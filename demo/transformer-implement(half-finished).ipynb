{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3485,"status":"ok","timestamp":1673581528619,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"G29OHZnE9YKa"},"outputs":[],"source":["!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21972,"status":"ok","timestamp":1673581550585,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"tT9uImTbBxKV","outputId":"fffa4da1-3a37-4d46-b204-5746d393e824"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting de-core-news-sm==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from de-core-news-sm==3.2.0) (3.2.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.64.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.7)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.11.3)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.17)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.9)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.8)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.9)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (57.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.25.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.21.6)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.10.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (6.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2022.12.7)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.4)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.1)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.9)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.9)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.11)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (6.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python -m spacy download de_core_news_sm\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6632,"status":"ok","timestamp":1673581557189,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"Okvz58S0Mb81"},"outputs":[],"source":["import os\n","from os.path import exists\n","import torch\n","import torch.nn as nn\n","from torch.nn.functional import log_softmax, pad\n","import math\n","import copy\n","import time\n","from torch.optim.lr_scheduler import LambdaLR\n","import pandas as pd\n","import altair as alt\n","from torchtext.data.functional import to_map_style_dataset\n","from torch.utils.data import DataLoader\n","from torchtext.vocab import build_vocab_from_iterator\n","import torchtext.datasets as datasets\n","import spacy\n","import GPUtil\n","import warnings\n","from torch.utils.data.distributed import DistributedSampler\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","from torch.nn.parallel import DistributedDataParallel as DDP"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":113,"status":"ok","timestamp":1673581557190,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"wiWYagSTEAro"},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    一个标准的解码器，编码器模型\n","    \"\"\"\n","\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.generator = generator\n","\n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        \"接收处理屏蔽的src和目标序列\"\n","        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n","\n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","\n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n","\n","class Generator(nn.Module):\n","    \"\"\"\n","    定义一个标准的线性+softmax生成步骤。\n","    说人话，这个是用来接受最后的decode的结果，并且返回词典中每个词的概率\n","    \"\"\"\n","    def __init__(self, d_model, vocab):\n","        super().__init__()\n","        self.proj = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        \"为什么用log_softmax，主要是梯度和计算速度的考虑，可以百度一下，资料很多\"\n","        return log_softmax(self.proj(x), dim=-1)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":123,"status":"ok","timestamp":1673581557202,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"2LYCgSOCYYVn"},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    def __init__(self, features, eps = 1e-5):\n","        super().__init__()\n","        # self.norm = nn.LayerNorm()\n","        # Add into parameters of modules\n","        self.gamma = nn.Parameter(torch.ones(features))\n","        self.beta = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim = True) \n","        # std = x.std(-1, keepdim = True)\n","        return self.gamma * (x - mean) / torch.sqrt(torch.var(x, unbiased = False) + eps) + self.beta\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":122,"status":"ok","timestamp":1673581557202,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"sQV4OwQHhfEt"},"outputs":[],"source":["class AddNorm(nn.Module):\n","    def __init__(self, size, dropout):\n","        super().__init__()\n","        # self.norm = nn.LayerNorm()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self, x, sublayer):\n","        return x + self.dropout(sublayer(self.norm(x)))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":122,"status":"ok","timestamp":1673581557203,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"mrhQYDaMVHfA"},"outputs":[],"source":["def attention(query, key, value, mask = None, dropout = None):\n","    # Choose the last dim of query\n","    d_k = query.size(-1)\n","    # Convert the last 2 dims to get K_t\n","    scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(d_k)\n","    if mask is not None:\n","    # Mask the pos with -1e9 where the status is True\n","        scores = scores.masked_fill_(mask == 0, -1e9)\n","    p_attn = scores.softmax(dim=-1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":118,"status":"ok","timestamp":1673581557204,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"03kF04D6RIag"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, layer, N):\n","        super().__init__()\n","        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n","        # layer\n","        self.norm = LayerNorm(layer.size)\n","    def foward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":114,"status":"ok","timestamp":1673581557204,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"TFsNjoDEiTs-"},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, size, self_attention, feed_forward, dropout):\n","        super().__init__()\n","        self.size = size\n","        self.self_attention = self_attention\n","        self.feed_forward = feed_forward\n","        self.layer1 = AddNorm(size, dropout)\n","        self.layer2 = AddNorm(size, dropout)\n","    def forward(self, x):\n","        x = self.layer1(x, lambda x: self.self_attention(x, x, x, mask))\n","        return self.layer2(x, self.feed_forward)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":114,"status":"ok","timestamp":1673581557205,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"rkNXDjA9VG_L"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, layer, N):\n","        super().__init__()\n","        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n","        # layer\n","        self.norm = LayerNorm(layer.size)\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":113,"status":"ok","timestamp":1673581557205,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"G934X4gqgqkF"},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super().__init__()\n","        self.size = size\n","        self.self_attention = self_attn\n","        self.cross_attention = src_attn\n","        self.feed_forward = feed_forward\n","        self.layer1 = AddNorm(size, dropout)\n","        self.layer2 = AddNorm(size, dropout)\n","        self.layer3 = AddNorm(size, dropout)\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        x = self.layer1(x, lambda x: self.self_attention(x, x, x, mask))\n","        x = self.layer2(x, lambda x: self.cross_attention(x, m, m, src_mask))\n","        return self.layer3(x, self.feed_forward)\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":115,"status":"ok","timestamp":1673581557207,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"7qWjAlRR8vUL"},"outputs":[],"source":["def subsequent_mask(size):\n","    #\"屏蔽后面的位置\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n","        torch.uint8\n","    )\n","    return subsequent_mask == 0"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":115,"status":"ok","timestamp":1673581557208,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"fiDg-M1E8xIp"},"outputs":[],"source":["def example_mask():\n","    # 第一眼看这个嵌套循环给看懵了。其实就是用两个for循环生成了一个二维坐标，每一个都是一个df对象\n","    # 看下面这个就好理解了\n","    # 其实:=[(x,y) for y in range(20) for x in range(20)]\n","\n","    LS_data = pd.concat(\n","        [\n","            pd.DataFrame(\n","                {\n","                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n","                    \"Window\": y,\n","                    \"Masking\": x,\n","                }\n","            )\n","            for y in range(20)\n","            for x in range(20)\n","        ]\n","    )\n","    return (\n","        alt.Chart(LS_data)\n","        .mark_rect()\n","        .properties(height=250, width=250)\n","        .encode(\n","            alt.X(\"Window:O\"),\n","            alt.Y(\"Masking:O\"),\n","            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n","        )\n","        .interactive()\n","    )"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1673581557208,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"MrF1s3O23Stm"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        super().__init__()\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","        self.attn = attention\n","        self.dropout = nn.Dropout(p=dropout)\n","    def forward(self, query, key, value, mask=None):\n","        if mask is not None:\n","            # Use the same mask for every head\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        # Adjust the output size to simulate heads\n","        # 这里坐了一个等价变换，通过改变输出的形状模拟了多个输出head\n","        query = self.w_q(query).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","        key = self.w_k(key).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","        value = self.w_v(value).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","        x, p_atten = self.attn(query, key, value, mask = mask, dropout=self.dropout)\n","        # Concatenate heads into one head\n","        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n","        return self.w_o(x)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":110,"status":"ok","timestamp":1673581557209,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"XeT4lM8Zm2T7"},"outputs":[],"source":["class PositionalWisedFFN(nn.Module):\n","  # 前馈神经网络\n","    def __init__(self, d_model, hidden_size, dropout=0.1):\n","        super().__init__()\n","        self.w1 = nn.Linear(d_model, hidden_size)\n","        self.w2 = nn.Linear(hidden_size, d_model)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self, x):\n","        return self.w2(self.dropout(self.relu(self.w1(x))))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":109,"status":"ok","timestamp":1673581557209,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"7BeSfLx312Kx"},"outputs":[],"source":["class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        super().__init__()\n","        self.emd = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        return self.emd(x) * math.sqrt(self.d_model)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":109,"status":"ok","timestamp":1673581557210,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"pzZyP8Y37zP3"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # 在对数空间中计算位置编码。\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(\n","            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n","        )\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        # 为不属于模型参数的状态增加缓冲\n","        self.register_buffer(\"pe\", pe)\n","\n","    def forward(self, x):\n","        # requires_grad_(False)：禁用梯度下降\n","        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":108,"status":"ok","timestamp":1673581557211,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"y1HlWua88WKR"},"outputs":[],"source":["def example_positional():\n","    pe = PositionalEncoding(20, 0)\n","    y = pe.forward(torch.zeros(1, 100, 20))\n","\n","    data = pd.concat(\n","        [\n","            pd.DataFrame(\n","                {\n","                    \"embedding\": y[0, :, dim],\n","                    \"dimension\": dim,\n","                    \"position\": list(range(100)),\n","                }\n","            )\n","            for dim in [4, 5, 6, 7]\n","        ]\n","    )\n","\n","    return (\n","        alt.Chart(data)\n","        .mark_line()\n","        .properties(width=800)\n","        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n","        .interactive()\n","    )\n","\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":108,"status":"ok","timestamp":1673581557215,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"53hBoetN8mjM"},"outputs":[],"source":["RUN_EXAMPLES = True\n","def show_example(fn, args=[]):\n","    if __name__ == \"__main__\" and RUN_EXAMPLES:\n","        return fn(*args)\n","\n","\n","def execute_example(fn, args=[]):\n","    if __name__ == \"__main__\" and RUN_EXAMPLES:\n","        fn(*args)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1673581557219,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"qSuGpDeT9EdM","outputId":"c1118a16-a09d-4e6b-94ab-2b4d005050f1"},"outputs":[{"data":{"text/html":["\n","<div id=\"altair-viz-458123f2414f483d8dcd00c90b1dfa13\"></div>\n","<script type=\"text/javascript\">\n","  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n","  (function(spec, embedOpt){\n","    let outputDiv = document.currentScript.previousElementSibling;\n","    if (outputDiv.id !== \"altair-viz-458123f2414f483d8dcd00c90b1dfa13\") {\n","      outputDiv = document.getElementById(\"altair-viz-458123f2414f483d8dcd00c90b1dfa13\");\n","    }\n","    const paths = {\n","      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n","      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n","      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n","      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n","    };\n","\n","    function maybeLoadScript(lib, version) {\n","      var key = `${lib.replace(\"-\", \"\")}_version`;\n","      return (VEGA_DEBUG[key] == version) ?\n","        Promise.resolve(paths[lib]) :\n","        new Promise(function(resolve, reject) {\n","          var s = document.createElement('script');\n","          document.getElementsByTagName(\"head\")[0].appendChild(s);\n","          s.async = true;\n","          s.onload = () => {\n","            VEGA_DEBUG[key] = version;\n","            return resolve(paths[lib]);\n","          };\n","          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n","          s.src = paths[lib];\n","        });\n","    }\n","\n","    function showError(err) {\n","      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n","      throw err;\n","    }\n","\n","    function displayChart(vegaEmbed) {\n","      vegaEmbed(outputDiv, spec, embedOpt)\n","        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n","    }\n","\n","    if(typeof define === \"function\" && define.amd) {\n","      requirejs.config({paths});\n","      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n","    } else {\n","      maybeLoadScript(\"vega\", \"5\")\n","        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n","        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n","        .catch(showError)\n","        .then(() => displayChart(vegaEmbed));\n","    }\n","  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-eafc635ddfa311c913cbcc2d2bc4477d\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-eafc635ddfa311c913cbcc2d2bc4477d\": [{\"embedding\": 0.0, \"dimension\": 4, \"position\": 0}, {\"embedding\": 0.15782663226127625, \"dimension\": 4, \"position\": 1}, {\"embedding\": 0.3116971552371979, \"dimension\": 4, \"position\": 2}, {\"embedding\": 0.45775455236434937, \"dimension\": 4, \"position\": 3}, {\"embedding\": 0.5923377275466919, \"dimension\": 4, \"position\": 4}, {\"embedding\": 0.7120732069015503, \"dimension\": 4, \"position\": 5}, {\"embedding\": 0.813959538936615, \"dimension\": 4, \"position\": 6}, {\"embedding\": 0.8954429626464844, \"dimension\": 4, \"position\": 7}, {\"embedding\": 0.9544808864593506, \"dimension\": 4, \"position\": 8}, {\"embedding\": 0.989593505859375, \"dimension\": 4, \"position\": 9}, {\"embedding\": 0.9999006390571594, \"dimension\": 4, \"position\": 10}, {\"embedding\": 0.9851439595222473, \"dimension\": 4, \"position\": 11}, {\"embedding\": 0.94569331407547, \"dimension\": 4, \"position\": 12}, {\"embedding\": 0.8825376033782959, \"dimension\": 4, \"position\": 13}, {\"embedding\": 0.7972599267959595, \"dimension\": 4, \"position\": 14}, {\"embedding\": 0.6919978260993958, \"dimension\": 4, \"position\": 15}, {\"embedding\": 0.5693899393081665, \"dimension\": 4, \"position\": 16}, {\"embedding\": 0.4325096309185028, \"dimension\": 4, \"position\": 17}, {\"embedding\": 0.284787654876709, \"dimension\": 4, \"position\": 18}, {\"embedding\": 0.12992730736732483, \"dimension\": 4, \"position\": 19}, {\"embedding\": -0.028190065175294876, \"dimension\": 4, \"position\": 20}, {\"embedding\": -0.18560057878494263, \"dimension\": 4, \"position\": 21}, {\"embedding\": -0.3383587896823883, \"dimension\": 4, \"position\": 22}, {\"embedding\": -0.4826357960700989, \"dimension\": 4, \"position\": 23}, {\"embedding\": -0.6148146390914917, \"dimension\": 4, \"position\": 24}, {\"embedding\": -0.7315824031829834, \"dimension\": 4, \"position\": 25}, {\"embedding\": -0.8300122618675232, \"dimension\": 4, \"position\": 26}, {\"embedding\": -0.9076365828514099, \"dimension\": 4, \"position\": 27}, {\"embedding\": -0.96250981092453, \"dimension\": 4, \"position\": 28}, {\"embedding\": -0.9932565093040466, \"dimension\": 4, \"position\": 29}, {\"embedding\": -0.9991058707237244, \"dimension\": 4, \"position\": 30}, {\"embedding\": -0.9799113273620605, \"dimension\": 4, \"position\": 31}, {\"embedding\": -0.9361540079116821, \"dimension\": 4, \"position\": 32}, {\"embedding\": -0.8689308166503906, \"dimension\": 4, \"position\": 33}, {\"embedding\": -0.7799267172813416, \"dimension\": 4, \"position\": 34}, {\"embedding\": -0.6713724136352539, \"dimension\": 4, \"position\": 35}, {\"embedding\": -0.5459895133972168, \"dimension\": 4, \"position\": 36}, {\"embedding\": -0.40692076086997986, \"dimension\": 4, \"position\": 37}, {\"embedding\": -0.2576519548892975, \"dimension\": 4, \"position\": 38}, {\"embedding\": -0.10192479938268661, \"dimension\": 4, \"position\": 39}, {\"embedding\": 0.056357722729444504, \"dimension\": 4, \"position\": 40}, {\"embedding\": 0.21322709321975708, \"dimension\": 4, \"position\": 41}, {\"embedding\": 0.3647516369819641, \"dimension\": 4, \"position\": 42}, {\"embedding\": 0.5071332454681396, \"dimension\": 4, \"position\": 43}, {\"embedding\": 0.6368028521537781, \"dimension\": 4, \"position\": 44}, {\"embedding\": 0.7505101561546326, \"dimension\": 4, \"position\": 45}, {\"embedding\": 0.8454052805900574, \"dimension\": 4, \"position\": 46}, {\"embedding\": 0.9191088080406189, \"dimension\": 4, \"position\": 47}, {\"embedding\": 0.9697737693786621, \"dimension\": 4, \"position\": 48}, {\"embedding\": 0.9961300492286682, \"dimension\": 4, \"position\": 49}, {\"embedding\": 0.9975170493125916, \"dimension\": 4, \"position\": 50}, {\"embedding\": 0.9738998413085938, \"dimension\": 4, \"position\": 51}, {\"embedding\": 0.9258706569671631, \"dimension\": 4, \"position\": 52}, {\"embedding\": 0.8546332716941833, \"dimension\": 4, \"position\": 53}, {\"embedding\": 0.7619734406471252, \"dimension\": 4, \"position\": 54}, {\"embedding\": 0.6502137184143066, \"dimension\": 4, \"position\": 55}, {\"embedding\": 0.5221555233001709, \"dimension\": 4, \"position\": 56}, {\"embedding\": 0.38100889325141907, \"dimension\": 4, \"position\": 57}, {\"embedding\": 0.23031172156333923, \"dimension\": 4, \"position\": 58}, {\"embedding\": 0.07384055852890015, \"dimension\": 4, \"position\": 59}, {\"embedding\": -0.08448058366775513, \"dimension\": 4, \"position\": 60}, {\"embedding\": -0.2406841218471527, \"dimension\": 4, \"position\": 61}, {\"embedding\": -0.3908545970916748, \"dimension\": 4, \"position\": 62}, {\"embedding\": -0.5312277674674988, \"dimension\": 4, \"position\": 63}, {\"embedding\": -0.6582850813865662, \"dimension\": 4, \"position\": 64}, {\"embedding\": -0.768841564655304, \"dimension\": 4, \"position\": 65}, {\"embedding\": -0.8601260781288147, \"dimension\": 4, \"position\": 66}, {\"embedding\": -0.9298503994941711, \"dimension\": 4, \"position\": 67}, {\"embedding\": -0.9762668013572693, \"dimension\": 4, \"position\": 68}, {\"embedding\": -0.9982118010520935, \"dimension\": 4, \"position\": 69}, {\"embedding\": -0.9951352477073669, \"dimension\": 4, \"position\": 70}, {\"embedding\": -0.967114269733429, \"dimension\": 4, \"position\": 71}, {\"embedding\": -0.9148513078689575, \"dimension\": 4, \"position\": 72}, {\"embedding\": -0.839656412601471, \"dimension\": 4, \"position\": 73}, {\"embedding\": -0.7434144616127014, \"dimension\": 4, \"position\": 74}, {\"embedding\": -0.6285378336906433, \"dimension\": 4, \"position\": 75}, {\"embedding\": -0.4979061186313629, \"dimension\": 4, \"position\": 76}, {\"embedding\": -0.3547937273979187, \"dimension\": 4, \"position\": 77}, {\"embedding\": -0.20278796553611755, \"dimension\": 4, \"position\": 78}, {\"embedding\": -0.04569905996322632, \"dimension\": 4, \"position\": 79}, {\"embedding\": 0.11253630369901657, \"dimension\": 4, \"position\": 80}, {\"embedding\": 0.2679498493671417, \"dimension\": 4, \"position\": 81}, {\"embedding\": 0.4166468679904938, \"dimension\": 4, \"position\": 82}, {\"embedding\": 0.5549001097679138, \"dimension\": 4, \"position\": 83}, {\"embedding\": 0.6792440414428711, \"dimension\": 4, \"position\": 84}, {\"embedding\": 0.7865618467330933, \"dimension\": 4, \"position\": 85}, {\"embedding\": 0.8741634488105774, \"dimension\": 4, \"position\": 86}, {\"embedding\": 0.9398530125617981, \"dimension\": 4, \"position\": 87}, {\"embedding\": 0.9819839596748352, \"dimension\": 4, \"position\": 88}, {\"embedding\": 0.9995002150535583, \"dimension\": 4, \"position\": 89}, {\"embedding\": 0.9919626712799072, \"dimension\": 4, \"position\": 90}, {\"embedding\": 0.959559977054596, \"dimension\": 4, \"position\": 91}, {\"embedding\": 0.903104841709137, \"dimension\": 4, \"position\": 92}, {\"embedding\": 0.8240122199058533, \"dimension\": 4, \"position\": 93}, {\"embedding\": 0.7242646217346191, \"dimension\": 4, \"position\": 94}, {\"embedding\": 0.6063624024391174, \"dimension\": 4, \"position\": 95}, {\"embedding\": 0.4732609689235687, \"dimension\": 4, \"position\": 96}, {\"embedding\": 0.32829657196998596, \"dimension\": 4, \"position\": 97}, {\"embedding\": 0.17510302364826202, \"dimension\": 4, \"position\": 98}, {\"embedding\": 0.01752028614282608, \"dimension\": 4, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 5, \"position\": 0}, {\"embedding\": 0.9874668121337891, \"dimension\": 5, \"position\": 1}, {\"embedding\": 0.9501814842224121, \"dimension\": 5, \"position\": 2}, {\"embedding\": 0.8890786170959473, \"dimension\": 5, \"position\": 3}, {\"embedding\": 0.8056897521018982, \"dimension\": 5, \"position\": 4}, {\"embedding\": 0.7021052241325378, \"dimension\": 5, \"position\": 5}, {\"embedding\": 0.5809215903282166, \"dimension\": 5, \"position\": 6}, {\"embedding\": 0.44517630338668823, \"dimension\": 5, \"position\": 7}, {\"embedding\": 0.2982720732688904, \"dimension\": 5, \"position\": 8}, {\"embedding\": 0.14389123022556305, \"dimension\": 5, \"position\": 9}, {\"embedding\": -0.01409643329679966, \"dimension\": 5, \"position\": 10}, {\"embedding\": -0.1717306226491928, \"dimension\": 5, \"position\": 11}, {\"embedding\": -0.32506027817726135, \"dimension\": 5, \"position\": 12}, {\"embedding\": -0.47024187445640564, \"dimension\": 5, \"position\": 13}, {\"embedding\": -0.6036361455917358, \"dimension\": 5, \"position\": 14}, {\"embedding\": -0.7218996286392212, \"dimension\": 5, \"position\": 15}, {\"embedding\": -0.8220675587654114, \"dimension\": 5, \"position\": 16}, {\"embedding\": -0.9016293287277222, \"dimension\": 5, \"position\": 17}, {\"embedding\": -0.9585906267166138, \"dimension\": 5, \"position\": 18}, {\"embedding\": -0.9915235042572021, \"dimension\": 5, \"position\": 19}, {\"embedding\": -0.9996025562286377, \"dimension\": 5, \"position\": 20}, {\"embedding\": -0.9826252460479736, \"dimension\": 5, \"position\": 21}, {\"embedding\": -0.941017210483551, \"dimension\": 5, \"position\": 22}, {\"embedding\": -0.8758211731910706, \"dimension\": 5, \"position\": 23}, {\"embedding\": -0.788671612739563, \"dimension\": 5, \"position\": 24}, {\"embedding\": -0.6817529797554016, \"dimension\": 5, \"position\": 25}, {\"embedding\": -0.5577451586723328, \"dimension\": 5, \"position\": 26}, {\"embedding\": -0.4197568893432617, \"dimension\": 5, \"position\": 27}, {\"embedding\": -0.27124688029289246, \"dimension\": 5, \"position\": 28}, {\"embedding\": -0.11593768745660782, \"dimension\": 5, \"position\": 29}, {\"embedding\": 0.042278096079826355, \"dimension\": 5, \"position\": 30}, {\"embedding\": 0.19943365454673767, \"dimension\": 5, \"position\": 31}, {\"embedding\": 0.3515901565551758, \"dimension\": 5, \"position\": 32}, {\"embedding\": 0.4949335753917694, \"dimension\": 5, \"position\": 33}, {\"embedding\": 0.6258708238601685, \"dimension\": 5, \"position\": 34}, {\"embedding\": 0.7411201596260071, \"dimension\": 5, \"position\": 35}, {\"embedding\": 0.8377919793128967, \"dimension\": 5, \"position\": 36}, {\"embedding\": 0.9134634733200073, \"dimension\": 5, \"position\": 37}, {\"embedding\": 0.9662377834320068, \"dimension\": 5, \"position\": 38}, {\"embedding\": 0.994792103767395, \"dimension\": 5, \"position\": 39}, {\"embedding\": 0.9984106421470642, \"dimension\": 5, \"position\": 40}, {\"embedding\": 0.9770026803016663, \"dimension\": 5, \"position\": 41}, {\"embedding\": 0.931104838848114, \"dimension\": 5, \"position\": 42}, {\"embedding\": 0.8618676662445068, \"dimension\": 5, \"position\": 43}, {\"embedding\": 0.7710266709327698, \"dimension\": 5, \"position\": 44}, {\"embedding\": 0.6608588695526123, \"dimension\": 5, \"position\": 45}, {\"embedding\": 0.5341253876686096, \"dimension\": 5, \"position\": 46}, {\"embedding\": 0.3940037488937378, \"dimension\": 5, \"position\": 47}, {\"embedding\": 0.24400585889816284, \"dimension\": 5, \"position\": 48}, {\"embedding\": 0.08789165318012238, \"dimension\": 5, \"position\": 49}, {\"embedding\": -0.07042567431926727, \"dimension\": 5, \"position\": 50}, {\"embedding\": -0.2269781529903412, \"dimension\": 5, \"position\": 51}, {\"embedding\": -0.37784066796302795, \"dimension\": 5, \"position\": 52}, {\"embedding\": -0.5192320942878723, \"dimension\": 5, \"position\": 53}, {\"embedding\": -0.6476082801818848, \"dimension\": 5, \"position\": 54}, {\"embedding\": -0.7597513794898987, \"dimension\": 5, \"position\": 55}, {\"embedding\": -0.8528502583503723, \"dimension\": 5, \"position\": 56}, {\"embedding\": -0.9245713949203491, \"dimension\": 5, \"position\": 57}, {\"embedding\": -0.973116934299469, \"dimension\": 5, \"position\": 58}, {\"embedding\": -0.9972700476646423, \"dimension\": 5, \"position\": 59}, {\"embedding\": -0.9964250922203064, \"dimension\": 5, \"position\": 60}, {\"embedding\": -0.9706035256385803, \"dimension\": 5, \"position\": 61}, {\"embedding\": -0.9204524159431458, \"dimension\": 5, \"position\": 62}, {\"embedding\": -0.8472290635108948, \"dimension\": 5, \"position\": 63}, {\"embedding\": -0.7527687549591064, \"dimension\": 5, \"position\": 64}, {\"embedding\": -0.6394393444061279, \"dimension\": 5, \"position\": 65}, {\"embedding\": -0.5100815296173096, \"dimension\": 5, \"position\": 66}, {\"embedding\": -0.3679378628730774, \"dimension\": 5, \"position\": 67}, {\"embedding\": -0.2165713608264923, \"dimension\": 5, \"position\": 68}, {\"embedding\": -0.05977622792124748, \"dimension\": 5, \"position\": 69}, {\"embedding\": 0.09851823002099991, \"dimension\": 5, \"position\": 70}, {\"embedding\": 0.254342257976532, \"dimension\": 5, \"position\": 71}, {\"embedding\": 0.4037908613681793, \"dimension\": 5, \"position\": 72}, {\"embedding\": 0.543117880821228, \"dimension\": 5, \"position\": 73}, {\"embedding\": 0.6688309907913208, \"dimension\": 5, \"position\": 74}, {\"embedding\": 0.7777789831161499, \"dimension\": 5, \"position\": 75}, {\"embedding\": 0.8672309517860413, \"dimension\": 5, \"position\": 76}, {\"embedding\": 0.9349446296691895, \"dimension\": 5, \"position\": 77}, {\"embedding\": 0.9792226552963257, \"dimension\": 5, \"position\": 78}, {\"embedding\": 0.998955249786377, \"dimension\": 5, \"position\": 79}, {\"embedding\": 0.9936476349830627, \"dimension\": 5, \"position\": 80}, {\"embedding\": 0.9634328484535217, \"dimension\": 5, \"position\": 81}, {\"embedding\": 0.9090684056282043, \"dimension\": 5, \"position\": 82}, {\"embedding\": 0.8319169878959656, \"dimension\": 5, \"position\": 83}, {\"embedding\": 0.733912467956543, \"dimension\": 5, \"position\": 84}, {\"embedding\": 0.617511510848999, \"dimension\": 5, \"position\": 85}, {\"embedding\": 0.4856317937374115, \"dimension\": 5, \"position\": 86}, {\"embedding\": 0.3415791094303131, \"dimension\": 5, \"position\": 87}, {\"embedding\": 0.18896427750587463, \"dimension\": 5, \"position\": 88}, {\"embedding\": 0.0316128134727478, \"dimension\": 5, \"position\": 89}, {\"embedding\": -0.12653106451034546, \"dimension\": 5, \"position\": 90}, {\"embedding\": -0.28150418400764465, \"dimension\": 5, \"position\": 91}, {\"embedding\": -0.4294200837612152, \"dimension\": 5, \"position\": 92}, {\"embedding\": -0.5665720105171204, \"dimension\": 5, \"position\": 93}, {\"embedding\": -0.6895220875740051, \"dimension\": 5, \"position\": 94}, {\"embedding\": -0.7951884269714355, \"dimension\": 5, \"position\": 95}, {\"embedding\": -0.880922257900238, \"dimension\": 5, \"position\": 96}, {\"embedding\": -0.9445747137069702, \"dimension\": 5, \"position\": 97}, {\"embedding\": -0.9845501184463501, \"dimension\": 5, \"position\": 98}, {\"embedding\": -0.9998465180397034, \"dimension\": 5, \"position\": 99}, {\"embedding\": 0.0, \"dimension\": 6, \"position\": 0}, {\"embedding\": 0.06305388361215591, \"dimension\": 6, \"position\": 1}, {\"embedding\": 0.12585683166980743, \"dimension\": 6, \"position\": 2}, {\"embedding\": 0.18815888464450836, \"dimension\": 6, \"position\": 3}, {\"embedding\": 0.24971213936805725, \"dimension\": 6, \"position\": 4}, {\"embedding\": 0.31027159094810486, \"dimension\": 6, \"position\": 5}, {\"embedding\": 0.3695962131023407, \"dimension\": 6, \"position\": 6}, {\"embedding\": 0.4274499714374542, \"dimension\": 6, \"position\": 7}, {\"embedding\": 0.4836025834083557, \"dimension\": 6, \"position\": 8}, {\"embedding\": 0.5378305912017822, \"dimension\": 6, \"position\": 9}, {\"embedding\": 0.5899181365966797, \"dimension\": 6, \"position\": 10}, {\"embedding\": 0.6396579146385193, \"dimension\": 6, \"position\": 11}, {\"embedding\": 0.6868520379066467, \"dimension\": 6, \"position\": 12}, {\"embedding\": 0.7313126921653748, \"dimension\": 6, \"position\": 13}, {\"embedding\": 0.7728629112243652, \"dimension\": 6, \"position\": 14}, {\"embedding\": 0.8113372921943665, \"dimension\": 6, \"position\": 15}, {\"embedding\": 0.8465827703475952, \"dimension\": 6, \"position\": 16}, {\"embedding\": 0.8784590363502502, \"dimension\": 6, \"position\": 17}, {\"embedding\": 0.9068393111228943, \"dimension\": 6, \"position\": 18}, {\"embedding\": 0.9316105246543884, \"dimension\": 6, \"position\": 19}, {\"embedding\": 0.9526742100715637, \"dimension\": 6, \"position\": 20}, {\"embedding\": 0.9699464440345764, \"dimension\": 6, \"position\": 21}, {\"embedding\": 0.9833585619926453, \"dimension\": 6, \"position\": 22}, {\"embedding\": 0.9928570985794067, \"dimension\": 6, \"position\": 23}, {\"embedding\": 0.9984043836593628, \"dimension\": 6, \"position\": 24}, {\"embedding\": 0.999978244304657, \"dimension\": 6, \"position\": 25}, {\"embedding\": 0.9975724220275879, \"dimension\": 6, \"position\": 26}, {\"embedding\": 0.9911965131759644, \"dimension\": 6, \"position\": 27}, {\"embedding\": 0.9808759093284607, \"dimension\": 6, \"position\": 28}, {\"embedding\": 0.9666516780853271, \"dimension\": 6, \"position\": 29}, {\"embedding\": 0.9485803842544556, \"dimension\": 6, \"position\": 30}, {\"embedding\": 0.9267339110374451, \"dimension\": 6, \"position\": 31}, {\"embedding\": 0.9011994004249573, \"dimension\": 6, \"position\": 32}, {\"embedding\": 0.8720782399177551, \"dimension\": 6, \"position\": 33}, {\"embedding\": 0.8394865393638611, \"dimension\": 6, \"position\": 34}, {\"embedding\": 0.8035537600517273, \"dimension\": 6, \"position\": 35}, {\"embedding\": 0.7644230127334595, \"dimension\": 6, \"position\": 36}, {\"embedding\": 0.7222501039505005, \"dimension\": 6, \"position\": 37}, {\"embedding\": 0.6772029399871826, \"dimension\": 6, \"position\": 38}, {\"embedding\": 0.6294605135917664, \"dimension\": 6, \"position\": 39}, {\"embedding\": 0.57921302318573, \"dimension\": 6, \"position\": 40}, {\"embedding\": 0.5266605615615845, \"dimension\": 6, \"position\": 41}, {\"embedding\": 0.4720119535923004, \"dimension\": 6, \"position\": 42}, {\"embedding\": 0.41548484563827515, \"dimension\": 6, \"position\": 43}, {\"embedding\": 0.3573042154312134, \"dimension\": 6, \"position\": 44}, {\"embedding\": 0.29770180583000183, \"dimension\": 6, \"position\": 45}, {\"embedding\": 0.23691439628601074, \"dimension\": 6, \"position\": 46}, {\"embedding\": 0.17518411576747894, \"dimension\": 6, \"position\": 47}, {\"embedding\": 0.1127568930387497, \"dimension\": 6, \"position\": 48}, {\"embedding\": 0.04988069087266922, \"dimension\": 6, \"position\": 49}, {\"embedding\": -0.013194027356803417, \"dimension\": 6, \"position\": 50}, {\"embedding\": -0.07621623575687408, \"dimension\": 6, \"position\": 51}, {\"embedding\": -0.1389348804950714, \"dimension\": 6, \"position\": 52}, {\"embedding\": -0.20110084116458893, \"dimension\": 6, \"position\": 53}, {\"embedding\": -0.2624664604663849, \"dimension\": 6, \"position\": 54}, {\"embedding\": -0.3227875530719757, \"dimension\": 6, \"position\": 55}, {\"embedding\": -0.3818237781524658, \"dimension\": 6, \"position\": 56}, {\"embedding\": -0.4393406808376312, \"dimension\": 6, \"position\": 57}, {\"embedding\": -0.49510911107063293, \"dimension\": 6, \"position\": 58}, {\"embedding\": -0.5489069223403931, \"dimension\": 6, \"position\": 59}, {\"embedding\": -0.6005204319953918, \"dimension\": 6, \"position\": 60}, {\"embedding\": -0.6497439742088318, \"dimension\": 6, \"position\": 61}, {\"embedding\": -0.6963817477226257, \"dimension\": 6, \"position\": 62}, {\"embedding\": -0.7402478456497192, \"dimension\": 6, \"position\": 63}, {\"embedding\": -0.7811681628227234, \"dimension\": 6, \"position\": 64}, {\"embedding\": -0.8189795017242432, \"dimension\": 6, \"position\": 65}, {\"embedding\": -0.8535317182540894, \"dimension\": 6, \"position\": 66}, {\"embedding\": -0.8846868872642517, \"dimension\": 6, \"position\": 67}, {\"embedding\": -0.9123212099075317, \"dimension\": 6, \"position\": 68}, {\"embedding\": -0.936324954032898, \"dimension\": 6, \"position\": 69}, {\"embedding\": -0.956602156162262, \"dimension\": 6, \"position\": 70}, {\"embedding\": -0.9730724096298218, \"dimension\": 6, \"position\": 71}, {\"embedding\": -0.9856699705123901, \"dimension\": 6, \"position\": 72}, {\"embedding\": -0.9943448305130005, \"dimension\": 6, \"position\": 73}, {\"embedding\": -0.9990625381469727, \"dimension\": 6, \"position\": 74}, {\"embedding\": -0.9998041391372681, \"dimension\": 6, \"position\": 75}, {\"embedding\": -0.9965668320655823, \"dimension\": 6, \"position\": 76}, {\"embedding\": -0.9893633723258972, \"dimension\": 6, \"position\": 77}, {\"embedding\": -0.9782225489616394, \"dimension\": 6, \"position\": 78}, {\"embedding\": -0.963188648223877, \"dimension\": 6, \"position\": 79}, {\"embedding\": -0.9443213939666748, \"dimension\": 6, \"position\": 80}, {\"embedding\": -0.9216960668563843, \"dimension\": 6, \"position\": 81}, {\"embedding\": -0.8954026699066162, \"dimension\": 6, \"position\": 82}, {\"embedding\": -0.8655455708503723, \"dimension\": 6, \"position\": 83}, {\"embedding\": -0.8322440981864929, \"dimension\": 6, \"position\": 84}, {\"embedding\": -0.795630156993866, \"dimension\": 6, \"position\": 85}, {\"embedding\": -0.7558501362800598, \"dimension\": 6, \"position\": 86}, {\"embedding\": -0.7130619883537292, \"dimension\": 6, \"position\": 87}, {\"embedding\": -0.6674357056617737, \"dimension\": 6, \"position\": 88}, {\"embedding\": -0.6191535592079163, \"dimension\": 6, \"position\": 89}, {\"embedding\": -0.5684073567390442, \"dimension\": 6, \"position\": 90}, {\"embedding\": -0.5153986215591431, \"dimension\": 6, \"position\": 91}, {\"embedding\": -0.46033912897109985, \"dimension\": 6, \"position\": 92}, {\"embedding\": -0.40344759821891785, \"dimension\": 6, \"position\": 93}, {\"embedding\": -0.3449500501155853, \"dimension\": 6, \"position\": 94}, {\"embedding\": -0.28508007526397705, \"dimension\": 6, \"position\": 95}, {\"embedding\": -0.22407560050487518, \"dimension\": 6, \"position\": 96}, {\"embedding\": -0.1621788740158081, \"dimension\": 6, \"position\": 97}, {\"embedding\": -0.09963719546794891, \"dimension\": 6, \"position\": 98}, {\"embedding\": -0.03669850528240204, \"dimension\": 6, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 7, \"position\": 0}, {\"embedding\": 0.9980100989341736, \"dimension\": 7, \"position\": 1}, {\"embedding\": 0.9920483827590942, \"dimension\": 7, \"position\": 2}, {\"embedding\": 0.9821385741233826, \"dimension\": 7, \"position\": 3}, {\"embedding\": 0.9683201313018799, \"dimension\": 7, \"position\": 4}, {\"embedding\": 0.9506479501724243, \"dimension\": 7, \"position\": 5}, {\"embedding\": 0.9291924834251404, \"dimension\": 7, \"position\": 6}, {\"embedding\": 0.9040390253067017, \"dimension\": 7, \"position\": 7}, {\"embedding\": 0.8752877116203308, \"dimension\": 7, \"position\": 8}, {\"embedding\": 0.8430529236793518, \"dimension\": 7, \"position\": 9}, {\"embedding\": 0.8074630498886108, \"dimension\": 7, \"position\": 10}, {\"embedding\": 0.7686597108840942, \"dimension\": 7, \"position\": 11}, {\"embedding\": 0.7267972826957703, \"dimension\": 7, \"position\": 12}, {\"embedding\": 0.6820423603057861, \"dimension\": 7, \"position\": 13}, {\"embedding\": 0.6345730423927307, \"dimension\": 7, \"position\": 14}, {\"embedding\": 0.5845783352851868, \"dimension\": 7, \"position\": 15}, {\"embedding\": 0.532257080078125, \"dimension\": 7, \"position\": 16}, {\"embedding\": 0.47781768441200256, \"dimension\": 7, \"position\": 17}, {\"embedding\": 0.4214765727519989, \"dimension\": 7, \"position\": 18}, {\"embedding\": 0.36345821619033813, \"dimension\": 7, \"position\": 19}, {\"embedding\": 0.30399325489997864, \"dimension\": 7, \"position\": 20}, {\"embedding\": 0.243318572640419, \"dimension\": 7, \"position\": 21}, {\"embedding\": 0.18167544901371002, \"dimension\": 7, \"position\": 22}, {\"embedding\": 0.11930941045284271, \"dimension\": 7, \"position\": 23}, {\"embedding\": 0.056468550115823746, \"dimension\": 7, \"position\": 24}, {\"embedding\": -0.006597157102078199, \"dimension\": 7, \"position\": 25}, {\"embedding\": -0.06963648647069931, \"dimension\": 7, \"position\": 26}, {\"embedding\": -0.1323987990617752, \"dimension\": 7, \"position\": 27}, {\"embedding\": -0.1946340948343277, \"dimension\": 7, \"position\": 28}, {\"embedding\": -0.25609490275382996, \"dimension\": 7, \"position\": 29}, {\"embedding\": -0.31653639674186707, \"dimension\": 7, \"position\": 30}, {\"embedding\": -0.37571826577186584, \"dimension\": 7, \"position\": 31}, {\"embedding\": -0.43340474367141724, \"dimension\": 7, \"position\": 32}, {\"embedding\": -0.4893665015697479, \"dimension\": 7, \"position\": 33}, {\"embedding\": -0.5433804988861084, \"dimension\": 7, \"position\": 34}, {\"embedding\": -0.5952321887016296, \"dimension\": 7, \"position\": 35}, {\"embedding\": -0.6447150111198425, \"dimension\": 7, \"position\": 36}, {\"embedding\": -0.6916319727897644, \"dimension\": 7, \"position\": 37}, {\"embedding\": -0.7357962727546692, \"dimension\": 7, \"position\": 38}, {\"embedding\": -0.7770324349403381, \"dimension\": 7, \"position\": 39}, {\"embedding\": -0.8151761889457703, \"dimension\": 7, \"position\": 40}, {\"embedding\": -0.8500756621360779, \"dimension\": 7, \"position\": 41}, {\"embedding\": -0.8815921545028687, \"dimension\": 7, \"position\": 42}, {\"embedding\": -0.9096000790596008, \"dimension\": 7, \"position\": 43}, {\"embedding\": -0.933988094329834, \"dimension\": 7, \"position\": 44}, {\"embedding\": -0.9546589255332947, \"dimension\": 7, \"position\": 45}, {\"embedding\": -0.971530556678772, \"dimension\": 7, \"position\": 46}, {\"embedding\": -0.9845356941223145, \"dimension\": 7, \"position\": 47}, {\"embedding\": -0.9936226010322571, \"dimension\": 7, \"position\": 48}, {\"embedding\": -0.998755156993866, \"dimension\": 7, \"position\": 49}, {\"embedding\": -0.9999129772186279, \"dimension\": 7, \"position\": 50}, {\"embedding\": -0.9970912933349609, \"dimension\": 7, \"position\": 51}, {\"embedding\": -0.9903014898300171, \"dimension\": 7, \"position\": 52}, {\"embedding\": -0.9795705676078796, \"dimension\": 7, \"position\": 53}, {\"embedding\": -0.964941143989563, \"dimension\": 7, \"position\": 54}, {\"embedding\": -0.9464714527130127, \"dimension\": 7, \"position\": 55}, {\"embedding\": -0.9242351651191711, \"dimension\": 7, \"position\": 56}, {\"embedding\": -0.8983205556869507, \"dimension\": 7, \"position\": 57}, {\"embedding\": -0.8688308000564575, \"dimension\": 7, \"position\": 58}, {\"embedding\": -0.8358834981918335, \"dimension\": 7, \"position\": 59}, {\"embedding\": -0.7996094226837158, \"dimension\": 7, \"position\": 60}, {\"embedding\": -0.7601531147956848, \"dimension\": 7, \"position\": 61}, {\"embedding\": -0.7176715731620789, \"dimension\": 7, \"position\": 62}, {\"embedding\": -0.6723340749740601, \"dimension\": 7, \"position\": 63}, {\"embedding\": -0.6243206262588501, \"dimension\": 7, \"position\": 64}, {\"embedding\": -0.5738227963447571, \"dimension\": 7, \"position\": 65}, {\"embedding\": -0.5210408568382263, \"dimension\": 7, \"position\": 66}, {\"embedding\": -0.46618568897247314, \"dimension\": 7, \"position\": 67}, {\"embedding\": -0.4094752371311188, \"dimension\": 7, \"position\": 68}, {\"embedding\": -0.3511347472667694, \"dimension\": 7, \"position\": 69}, {\"embedding\": -0.2913972735404968, \"dimension\": 7, \"position\": 70}, {\"embedding\": -0.23049965500831604, \"dimension\": 7, \"position\": 71}, {\"embedding\": -0.1686851680278778, \"dimension\": 7, \"position\": 72}, {\"embedding\": -0.10619935393333435, \"dimension\": 7, \"position\": 73}, {\"embedding\": -0.04329042136669159, \"dimension\": 7, \"position\": 74}, {\"embedding\": 0.019790323451161385, \"dimension\": 7, \"position\": 75}, {\"embedding\": 0.08279230445623398, \"dimension\": 7, \"position\": 76}, {\"embedding\": 0.14546526968479156, \"dimension\": 7, \"position\": 77}, {\"embedding\": 0.20755885541439056, \"dimension\": 7, \"position\": 78}, {\"embedding\": 0.2688263952732086, \"dimension\": 7, \"position\": 79}, {\"embedding\": 0.3290245532989502, \"dimension\": 7, \"position\": 80}, {\"embedding\": 0.3879128098487854, \"dimension\": 7, \"position\": 81}, {\"embedding\": 0.4452572762966156, \"dimension\": 7, \"position\": 82}, {\"embedding\": 0.5008301138877869, \"dimension\": 7, \"position\": 83}, {\"embedding\": 0.5544094443321228, \"dimension\": 7, \"position\": 84}, {\"embedding\": 0.605782687664032, \"dimension\": 7, \"position\": 85}, {\"embedding\": 0.6547446846961975, \"dimension\": 7, \"position\": 86}, {\"embedding\": 0.7011010050773621, \"dimension\": 7, \"position\": 87}, {\"embedding\": 0.7446674108505249, \"dimension\": 7, \"position\": 88}, {\"embedding\": 0.7852699160575867, \"dimension\": 7, \"position\": 89}, {\"embedding\": 0.8227472901344299, \"dimension\": 7, \"position\": 90}, {\"embedding\": 0.856950581073761, \"dimension\": 7, \"position\": 91}, {\"embedding\": 0.8877431154251099, \"dimension\": 7, \"position\": 92}, {\"embedding\": 0.9150027632713318, \"dimension\": 7, \"position\": 93}, {\"embedding\": 0.9386210441589355, \"dimension\": 7, \"position\": 94}, {\"embedding\": 0.9585037231445312, \"dimension\": 7, \"position\": 95}, {\"embedding\": 0.9745717644691467, \"dimension\": 7, \"position\": 96}, {\"embedding\": 0.9867613911628723, \"dimension\": 7, \"position\": 97}, {\"embedding\": 0.9950238466262817, \"dimension\": 7, \"position\": 98}, {\"embedding\": 0.9993264079093933, \"dimension\": 7, \"position\": 99}]}}, {\"mode\": \"vega-lite\"});\n","</script>"],"text/plain":["alt.Chart(...)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["show_example(example_positional)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":110,"status":"ok","timestamp":1673581557220,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"VY3mt-UW9mS6"},"outputs":[],"source":["def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    #\"帮助: 从超参数中构建一个模型\"\n","    c = copy.deepcopy\n","    attn = MultiHeadAttention(h, d_model)\n","    ff = PositionalWisedFFN(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab),\n","    )\n","\n","    # 这里很重要\n","    # 用Glorot/fan_avg初始化参数。\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","    return model"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":110,"status":"ok","timestamp":1673581557220,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"qmHJKZGa9rYR"},"outputs":[],"source":["class Batch:\n","    \"\"\"训练期间用于保存一批带掩码的数据的对象\"\"\"\n","\n","    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n","        self.src = src\n","        # 关于下面这一段：\n","        # (torch.tensor([[ 0, 2, 4, 5, 1, 0, 2 ]]) != 2).unsqueeze(-2)\n","        # print：tensor([[[ True, False,  True,  True,  True,  True, False]]])\n","        # 实际是把2元素打上掩码。\n","        self.src_mask = (src != pad).unsqueeze(-2)\n","        if tgt is not None:\n","            # 下面两个分别去掉句子的开始符和结束符，这两个符合不参与运算\n","            self.tgt = tgt[:, :-1]\n","            self.tgt_y = tgt[:, 1:]\n","            # 把输入指定位置的下三角掩码\n","            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n","            self.ntokens = (self.tgt_y != pad).data.sum()\n","\n","    @staticmethod\n","    def make_std_mask(tgt, pad):\n","        \"创建一个掩码来隐藏并填充未来的word\"\n","        # 和src一样，需要把<blank>符合盖住\n","        tgt_mask = (tgt != pad).unsqueeze(-2)\n","        # 取&操作\n","        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n","            tgt_mask.data\n","        )\n","        return tgt_mask"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":109,"status":"ok","timestamp":1673581557221,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"9ntJ-Hau94Yk"},"outputs":[],"source":["class TrainState:\n","    \"\"\"用来跟踪当前训练的情况，包括步数，梯度步数，样本使用数和已经处理的tokens数量\"\"\"\n","\n","    step: int = 0  # Steps in the current epoch\n","    accum_step: int = 0  # Number of gradient accumulation steps\n","    samples: int = 0  # total # of examples used\n","    tokens: int = 0  # total # of tokens processed"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":109,"status":"ok","timestamp":1673581557221,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"SzD1slam-ErN"},"outputs":[],"source":["def run_epoch(\n","    data_iter,\n","    model,\n","    loss_compute,\n","    optimizer,\n","    scheduler,\n","    mode=\"train\",\n","    accum_iter=1,\n","    train_state=TrainState(),\n","):\n","    \"\"\"一个训练epoch\n","    data_iter: 可迭代对象，一次返回一个Batch对象或者加上索引\n","    model:训练的模型，这里就是Transformer\n","    loss_compute: SimpleLossCompute对象，用于计算损失\n","    optimizer: 优化器。这里是Adam优化器。验证时，optimizer是DummyOptimizer。DummyOptimizer不会真的更新模型参数，主要用于不同优化器效果的对比。\n","    scheduler：执行控制器。scheduler是一种用于调整优化器学习率的工具。 它可以帮助我们在训练过程中根据指定的策略调整学习率，\n","      以提高模型的性能这里是LambdaLR对象，用于调整Adam的学习率，实现WarmUp；验证时，scheduler是DummyScheduler。\n","    accum_iter: 每迭代n个batch更新一次模型的参数。这里默认n=1，就是每次batch都更新参数。\n","    train_state: TrainState对象，用于保存前训练的情况\n","    \"\"\"\n","    start = time.time()\n","    total_tokens = 0\n","    total_loss = 0\n","    tokens = 0\n","    n_accum = 0\n","    for i, batch in enumerate(data_iter):\n","        # 注意这里的out是decoder输出的结果，这会还没有经过最后一层linear+softmax\n","        out = model.forward(\n","            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n","        )\n","        # 这里才传入out和训练目标tgt_y计算了loss和loss_node。loss_node返回的是正则化的损失；\n","        # loss用来计算损失，loss_node用来梯度下降更新参数\n","        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n","        # loss_node = loss_node / accum_iter\n","        # 只有在train或者train+log的模式才开启参数更新\n","        if mode == \"train\" or mode == \"train+log\":\n","            # 先通过backward计算出来梯度\n","            loss_node.backward()\n","            train_state.step += 1\n","            train_state.samples += batch.src.shape[0]\n","            train_state.tokens += batch.ntokens\n","            if i % accum_iter == 0:\n","                # 调用依次梯度下降\n","                optimizer.step()\n","                optimizer.zero_grad(set_to_none=True)\n","                n_accum += 1\n","                train_state.accum_step += 1\n","            # 我们在备注里提到过,scheduler的作用就是用来优化学习，控制学习率等超参数。这里调用step就是更新学习率相关的参数\n","            scheduler.step()\n","\n","        total_loss += loss\n","        total_tokens += batch.ntokens\n","        tokens += batch.ntokens\n","        \n","        # 下面是每40个epoch打印下相关日志\n","        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n","            # 学习率\n","            lr = optimizer.param_groups[0][\"lr\"]\n","            # 40个epoch花费的时间\n","            elapsed = time.time() - start\n","            print(\n","                (\n","                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n","                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n","                )\n","                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n","            )\n","            start = time.time()\n","            tokens = 0\n","        del loss\n","        del loss_node\n","    return total_loss / total_tokens, train_state"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":108,"status":"ok","timestamp":1673581557222,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"3HE1gzG--HjQ"},"outputs":[],"source":["def rate(step, model_size, factor, warmup):\n","    \"\"\"\n","    我们必须将LambdaLR函数的最小步数默认为1。以避免零点导致的负数学习率。\n","    \"\"\"\n","    if step == 0:\n","        step = 1\n","    return factor * (\n","        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n","    )"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":112,"status":"ok","timestamp":1673581557226,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"EQBwYHcq-KRn"},"outputs":[],"source":["def example_learning_schedule():\n","    opts = [\n","        [512, 1, 4000],  # example 1\n","        [512, 1, 8000],  # example 2\n","        [256, 1, 4000],  # example 3\n","    ]\n","\n","    dummy_model = torch.nn.Linear(1, 1)\n","    learning_rates = []\n","\n","    # 在我们的配置列表里有三个不同的案例.\n","    for idx, example in enumerate(opts):\n","        # 创建一个Adam优化器\n","        optimizer = torch.optim.Adam(\n","            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n","        )\n","        # 这里创建一个神经网络学习调度器，优化器就是刚才创建的Adam优化器，lr(学习率)的调整函数就是上面的rate函数，根据当前的step来调整。\n","        lr_scheduler = LambdaLR(\n","            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n","        )\n","        tmp = []\n","        # 采取20K假训练步骤，保存每一步的学习率\n","        for step in range(20000):\n","            # 把当前的学习率追加到等会输出的list\n","            tmp.append(optimizer.param_groups[0][\"lr\"])\n","            # optimizer.step()是更新网络的参数\n","            optimizer.step()\n","            # scheduler.step()是更新学习率等控制网络学习的参数\n","            lr_scheduler.step()\n","        learning_rates.append(tmp)\n","\n","    learning_rates = torch.tensor(learning_rates)\n","\n","    # 关掉最大限制，使得altair能够处理超过5000行的数据\n","    alt.data_transformers.disable_max_rows()\n","\n","    opts_data = pd.concat(\n","        [\n","            pd.DataFrame(\n","                {\n","                    \"Learning Rate\": learning_rates[warmup_idx, :],\n","                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n","                        warmup_idx\n","                    ],\n","                    \"step\": range(20000),\n","                }\n","            )\n","            for warmup_idx in [0, 1, 2]\n","        ]\n","    )\n","\n","    return (\n","        alt.Chart(opts_data)\n","        .mark_line()\n","        .properties(width=600)\n","        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n","        .interactive()\n","    )\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":114,"status":"ok","timestamp":1673581557232,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"YVfdi408-Vba"},"outputs":[],"source":["class LabelSmoothing(nn.Module):\n","    #\"实现标签平滑.\"\n","\n","    def __init__(self, size, padding_idx, smoothing=0.0):\n","        super(LabelSmoothing, self).__init__()\n","        # 定义一个KL散度loss网络，损失的计算方式是sum，求和\n","        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n","        self.padding_idx = padding_idx\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.size = size\n","        self.true_dist = None\n","\n","    def forward(self, x, target):\n","        assert x.size(1) == self.size\n","        # 克隆一份作为真实分布\n","        true_dist = x.data.clone()\n","        # 用smoothing/(self.size - 2) 填充\n","        true_dist.fill_(self.smoothing / (self.size - 2))\n","        # 用confidence填充指定位置的数据，scatter_用法参考[8]\n","        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        true_dist[:, self.padding_idx] = 0\n","        mask = torch.nonzero(target.data == self.padding_idx)\n","        if mask.dim() > 0:\n","            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n","        self.true_dist = true_dist\n","        # detach，把变量从计算图分离，可参考 https://zhuanlan.zhihu.com/p/389738863\n","        return self.criterion(x, true_dist.clone().detach())"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":110,"status":"ok","timestamp":1673581557232,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"lDk9tl_J-W84"},"outputs":[],"source":["class SimpleLossCompute:\n","    \"一个简单的损失计算和训练函数\"\n","\n","    def __init__(self, generator, criterion):\n","        \"\"\"\n","        generator: Generator对象，用于根据Decoder的输出预测token\n","        criterion: LabelSmoothing对象，用于对Label进行平滑处理和损失计算\n","        \"\"\"\n","        self.generator = generator\n","        self.criterion = criterion\n","\n","    def __call__(self, x, y, norm):\n","        # 这里顺便用最简单的例子展示了smoothing的用法\n","        # 先把decoder的x输入generator，得到预测的x\n","        # 然后把x和预测的y传入，criterion会对y做平滑处理，需要注意的是:\n","        # 这里传入的y展开成了一个一阶张量，即向量，因为在criterion内部会对它打包，会为每个单词生成一个概率向量\n","        x = self.generator(x)\n","        sloss = (\n","            self.criterion(\n","                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n","            )\n","            / norm\n","        )\n","        \n","        # 这里又在搞事情，相当于第一个没有norm,第二个sloss是norm版本的，除以的是一个常量,batch.ntokens\n","        return sloss.data * norm, sloss"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1673581557234,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"Zjm6D0vX-iCR"},"outputs":[],"source":["import heapq\n","\n","class Beam:\n","    def __init__(self, beam_width):\n","        self.heap = list()  # 存储各个beam search的结果\n","        self.beam_width = beam_width  # beam的数量\n","\n","    def add(self, probility, complete, seq, decoder_input, decoder_hidden):\n","        \"\"\"\n","        添加数据，同时判断总的数据个数，多则删除\n","        :param probility: 概率乘积\n","        :param complete: 最后一个是否为EOS\n","        :param seq: list，所有token的列表\n","        :param decoder_input: 下一次进行解码的输入，通过前一次获得\n","        :param decoder_hidden: 下一次进行解码的hidden，通过前一次获得\n","        :return:\n","        \"\"\"\n","        heapq.heappush(self.heap, [probility, complete,\n","                                   seq, decoder_input, decoder_hidden])\n","        # 判断数据的个数，如果大，则弹出。保证数据总个数小于等于beam_width\n","        if len(self.heap) > self.beam_width:\n","            heapq.heappop(self.heap)\n","\n","    def __iter__(self):  # 让该beam能够被迭代\n","        return iter(self.heap)\n","\n","\n","def beam_decode(model, src, src_mask, max_len, start_symbol, BEAM_SIZE):\n","    model.eval()\n","    beam_seq = Beam(BEAM_SIZE)\n","    # 构造第一次需要的输入数据，保存在堆中\n","    memory = model.encode(src, src_mask)\n","    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n","    out = model.decode(\n","        memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n","    )\n","    beam_seq.add(1, False, [ys], ys, out)\n","    while True:\n","        cur_beam = Beam(BEAM_SIZE)\n","        # 取出堆中的数据，进行forward_step的操作，获得当前时间步的output，hidden\n","        for _probility, _complete, _seq, _decoder_input, _decoder_hidden in beam_seq:\n","            # 判断前一次的_complete是否为True，如果是，则不需要forward\n","            if _complete == True:\n","                cur_beam.add(_probility, _complete, _seq,\n","                             _decoder_input, _decoder_hidden)\n","            else:\n","                decoder_hidden = model.decode(\n","                    memory, src_mask, _decoder_input, subsequent_mask(ys.size(1)).type_as(src.data))\n","                decoder_output_t = model.generator(decoder_hidden[:, -1])\n","                value, index = torch.topk(decoder_output_t, BEAM_SIZE)\n","                # 从output中选择topk（k=beam width）个输出，作为下一次的input\n","                for m, n in zip(value, index):\n","                    decoder_input = torch.LongTensor([[n[0]]])\n","                    decoder_input = torch.cat(\n","                        [_decoder_input, decoder_input], dim=1)\n","                    seq = _seq + [n[0]]\n","                    probility = _probility * m[0]\n","                    # probility = _probility + m\n","                    if n[0].item() == 1:  # index of </s>\n","                        complete = True\n","                    else:\n","                        complete = False\n","                    cur_beam.add(probility, complete, seq,\n","                                 decoder_input, decoder_hidden)\n","        # 获取新的堆中的优先级最高（概率最大）的数据，判断数据是否是EOS结尾或者是否达到最大长度，如果是，停止迭代\n","        best_prob, best_complete, best_seq, _, _ = max(cur_beam)\n","        if best_complete == True or len(best_seq) - 1 == max_len:  # 减去</s>\n","            seq = [i.item() for i in best_seq]\n","            return seq\n","            # return best_seq\n","        else:\n","            # 重新遍历新的堆中的数据\n","            beam_seq = cur_beam\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":112,"status":"ok","timestamp":1673581557235,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"ASi0_Q3x_Q2L"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":112,"status":"ok","timestamp":1673581557236,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"aTrLrd-h_yED"},"outputs":[],"source":["def load_tokenizers():\n","\n","    try:\n","        spacy_de = spacy.load(\"de_core_news_sm\")\n","    except IOError:\n","        os.system(\"python -m spacy download de_core_news_sm\")\n","        spacy_de = spacy.load(\"de_core_news_sm\")\n","\n","    try:\n","        spacy_en = spacy.load(\"en_core_web_sm\")\n","    except IOError:\n","        os.system(\"python -m spacy download en_core_web_sm\")\n","        spacy_en = spacy.load(\"en_core_web_sm\")\n","\n","    return spacy_de, spacy_en"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1673581557236,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"wkMNo1Cg_1dS"},"outputs":[],"source":["def tokenize(text, tokenizer):\n","    \"\"\"对text进行分词\n","    :param text: 要分词的文本，例如'I love you'\n","    :param tokenizer: 分词模型，例如：spacy_en\n","    :return: 分词结果，例如 [\"I\", \"love\", \"you\"]\n","    \"\"\"\n","    return [tok.text for tok in tokenizer.tokenizer(text)]\n","\n","\n","def yield_tokens(data_iter, tokenizer, index):\n","    \"\"\"yield一个Token List\n","    :param data_iter: 包含句子对的可迭代对象。例如：[(\"I love you\", \"我爱你\"), ...]\n","    :param tokenizer: 分词模型。例如spacy_en\n","    :param index: 要对句子对儿的哪个语言进行分词，例如0表示对上例的英文进行分词\n","    :return: yield本轮的分词结果，例如['I', 'love', 'you']\n","    \"\"\"\n","    for from_to_tuple in data_iter:\n","        yield tokenizer(from_to_tuple[index])"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1673581557237,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"sbQhlaqnALea"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1808,"status":"ok","timestamp":1673581558935,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"8UohWI4Q_3nJ","outputId":"5926563b-71da-4fa1-a387-9007d927fdac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished.\n","Vocabulary sizes:\n","8315\n","6384\n"]}],"source":["def build_vocabulary(spacy_de, spacy_en):\n","    \"\"\"\n","    构建德语词典和英语词典\n","    :return: 返回德语词典和英语词典，均为：Vocab对象\n","             Vocab对象官方地址为：https://pytorch.org/text/stable/vocab.html#vocab\n","    \"\"\"\n","    def tokenize_de(text):\n","        return tokenize(text, spacy_de)\n","\n","    def tokenize_en(text):\n","        return tokenize(text, spacy_en)\n","    \n","    # [9]build_vocab_from_iterator()函数\n","    print(\"Building German Vocabulary ...\")\n","    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n","    vocab_src = build_vocab_from_iterator(\n","        yield_tokens(train + val + test, tokenize_de, index=0),\n","        min_freq=2,\n","        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n","    )\n","\n","    print(\"Building English Vocabulary ...\")\n","    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n","    vocab_tgt = build_vocab_from_iterator(\n","        yield_tokens(train + val + test, tokenize_en, index=1),\n","        min_freq=2,\n","        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n","    )\n","\n","    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n","    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n","\n","    return vocab_src, vocab_tgt\n","\n","\n","def load_vocab(spacy_de, spacy_en):\n","    if not exists(\"vocab.pt\"):\n","        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n","        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n","    else:\n","        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n","    print(\"Finished.\\nVocabulary sizes:\")\n","    print(len(vocab_src))\n","    print(len(vocab_tgt))\n","    return vocab_src, vocab_tgt\n","\n","\n","# if is_interactive_notebook():\n","    # 均成为脚本之后的全局变量\n","spacy_de, spacy_en = show_example(load_tokenizers)\n","vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1673581558936,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"42JVQviPC3JN"},"outputs":[],"source":["class DummyOptimizer(torch.optim.Optimizer):\n","    def __init__(self):\n","        self.param_groups = [{\"lr\": 0}]\n","        None\n","\n","    def step(self):\n","        None\n","\n","    def zero_grad(self, set_to_none=False):\n","        None\n","\n","\n","class DummyScheduler:\n","    def step(self):\n","        None"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1673581558937,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"hNUt41FQCjBs"},"outputs":[],"source":["def collate_batch(\n","    batch,\n","    src_pipeline,\n","    tgt_pipeline,\n","    src_vocab,\n","    tgt_vocab,\n","    device,\n","    max_padding=128,\n","    pad_id=2,\n","):\n","    \"\"\"返回真正的训练批次张量，并且文本被['i', 'love', 'you']处理成[3,4,5]数字张量。\n","    :param batch: 一个batch的语句对。例如：\n","                  [('Ein Kleinkind ...', 'A toddler in ...'), # [(德语), (英语)\n","                   ....                                       # ...\n","                   ...]                                       # ... ]\n","    :param src_pipeline: 德语分词器，也就是tokenize_de方法，后面会定义其实就是对spacy_de的封装\n","    :param tgt_pipeline: 英语分词器，也就是tokenize_en方法\n","    :param src_vocab: 德语词典，Vocab对象\n","    :param tgt_vocab: 英语词典，Vocab对象\n","    :param device: cpu或cuda\n","    :param max_padding: 句子的长度。pad长度不足的句子和裁剪长度过长的句子，目的是让不同长度的句子可以组成一个tensor\n","    :param pad_id: '<blank>'在词典中对应的index\n","    :return: src和tgt。处理后并batch后的句子。例如：\n","             src为：[[0, 4354, 314, ..., 1, 2, 2, ..., 2],  [0, 4905, 8567, ..., 1, 2, 2, ..., 2]]\n","             其中0是<bos>, 1是<eos>, 2是<blank>；src的Shape为(batch_size, max_padding)；tgt同理。\n","    \"\"\"\n","    bs_id = torch.tensor([0], device=device)  # <s> token id\n","    eos_id = torch.tensor([1], device=device)  # </s> token id\n","    src_list, tgt_list = [], []\n","    for (_src, _tgt) in batch:\n","        processed_src = torch.cat(\n","            [\n","                bs_id,\n","                torch.tensor(\n","                    src_vocab(src_pipeline(_src)),\n","                    dtype=torch.int64,\n","                    device=device,\n","                ),\n","                eos_id,\n","            ],\n","            0,\n","        )\n","        processed_tgt = torch.cat(\n","            [\n","                bs_id,\n","                torch.tensor(\n","                    tgt_vocab(tgt_pipeline(_tgt)),\n","                    dtype=torch.int64,\n","                    device=device,\n","                ),\n","                eos_id,\n","            ],\n","            0,\n","        )\n","        src_list.append(\n","            # 警告 - 覆盖padding - len的负值的值\n","            pad(\n","                processed_src,\n","                (\n","                    0,\n","                    max_padding - len(processed_src),\n","                ),\n","                value=pad_id,\n","            )\n","        )\n","        tgt_list.append(\n","            pad(\n","                processed_tgt,\n","                (0, max_padding - len(processed_tgt)),\n","                value=pad_id,\n","            )\n","        )\n","\n","    src = torch.stack(src_list)\n","    tgt = torch.stack(tgt_list)\n","    return (src, tgt)"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1673581558937,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"SAIJv3rfCouq"},"outputs":[],"source":["def create_dataloaders(\n","    device,\n","    vocab_src,\n","    vocab_tgt,\n","    spacy_de,\n","    spacy_en,\n","    batch_size=12000,\n","    max_padding=128,\n","    is_distributed=False,\n","):\n","    \"\"\"创建一个dataloaders，实际上返回了两个，一个训练集，一个验证集的\n","    :param device: cpu或cuda\n","    :param vocab_src: 源词典，本例中为德语词典\n","    :param vocab_tgt: 目标词典，本例中为英语词典\n","    :param spacy_de: 德语分词器\n","    :param spacy_en: 英语分词器\n","    :param batch_size: 每个批次的样本量\n","    :param max_padding: 句子的最大长度。也是需要填充的长度。\n","    \n","    :return: 训练集dataloaders，验证集dataloaders\n","    \"\"\"\n","    # def create_dataloaders(batch_size=12000):\n","    def tokenize_de(text):\n","        return tokenize(text, spacy_de)\n","\n","    def tokenize_en(text):\n","        return tokenize(text, spacy_en)\n","\n","    def collate_fn(batch):\n","        return collate_batch(\n","            batch,\n","            tokenize_de,\n","            tokenize_en,\n","            vocab_src,\n","            vocab_tgt,\n","            device,\n","            max_padding=max_padding,\n","            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n","        )\n","\n","    train_iter, valid_iter, test_iter = datasets.Multi30k(\n","        language_pair=(\"de\", \"en\")\n","    )\n","\n","    train_iter_map = to_map_style_dataset(\n","        train_iter\n","    )  # DistributedSampler needs a dataset len()\n","    # 这里对sampler开启了分布式采样\n","    train_sampler = (\n","        DistributedSampler(train_iter_map) if is_distributed else None\n","    )\n","    valid_iter_map = to_map_style_dataset(valid_iter)\n","    valid_sampler = (\n","        DistributedSampler(valid_iter_map) if is_distributed else None\n","    )\n","    # [11]torch.utils.data.Dataloader()\n","    train_dataloader = DataLoader(\n","        train_iter_map,\n","        batch_size=batch_size,\n","        shuffle=(train_sampler is None),\n","        sampler=train_sampler,\n","        collate_fn=collate_fn,\n","    )\n","    valid_dataloader = DataLoader(\n","        valid_iter_map,\n","        batch_size=batch_size,\n","        shuffle=(valid_sampler is None),\n","        sampler=valid_sampler,\n","        collate_fn=collate_fn,\n","    )\n","    return train_dataloader, valid_dataloader"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1673581558938,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"v4BZxegbCrzh"},"outputs":[],"source":["def train_worker(\n","    gpu,\n","    ngpus_per_node,\n","    vocab_src,\n","    vocab_tgt,\n","    spacy_de,\n","    spacy_en,\n","    config,\n","    is_distributed=False,\n","):\n","    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n","    torch.cuda.set_device(gpu)\n","\n","    pad_idx = vocab_tgt[\"<blank>\"]\n","    d_model = 512\n","    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n","    model.cuda(gpu)\n","    module = model\n","    is_main_process = True\n","    if is_distributed:\n","        # 具体参考Q&A[10]\n","        dist.init_process_group(\n","            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n","        )\n","        # DDP->torch.nn.parallel.DistributedDataParallel:\n","        # 用于在分布式训练环境中在多个GPU或机器上并行训练一个模型。\n","        # 它的工作原理是在每个GPU或机器上复制模型，并在训练期间使用通信后端来同步模型的梯度和缓冲区。\n","        model = DDP(model, device_ids=[gpu])\n","        module = model.module\n","        is_main_process = gpu == 0\n","    \n","    # 创建一个标签平滑处理模型\n","    criterion = LabelSmoothing(\n","        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n","    )\n","    # 使用指定的GPU\n","    criterion.cuda(gpu)\n","    # 创建Dataloaders\n","    train_dataloader, valid_dataloader = create_dataloaders(\n","        gpu,\n","        vocab_src,\n","        vocab_tgt,\n","        spacy_de,\n","        spacy_en,\n","        batch_size=config[\"batch_size\"] // ngpus_per_node,\n","        max_padding=config[\"max_padding\"],\n","        is_distributed=is_distributed,\n","    )\n","\n","    optimizer = torch.optim.Adam(\n","        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n","    )\n","    lr_scheduler = LambdaLR(\n","        optimizer=optimizer,\n","        lr_lambda=lambda step: rate(\n","            step, d_model, factor=1, warmup=config[\"warmup\"]\n","        ),\n","    )\n","    train_state = TrainState()\n","\n","    for epoch in range(config[\"num_epochs\"]):\n","        if is_distributed:\n","            train_dataloader.sampler.set_epoch(epoch)\n","            valid_dataloader.sampler.set_epoch(epoch)\n","\n","        model.train()\n","        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n","        _, train_state = run_epoch(\n","            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n","            model,\n","            SimpleLossCompute(module.generator, criterion),\n","            optimizer,\n","            lr_scheduler,\n","            mode=\"train+log\",\n","            accum_iter=config[\"accum_iter\"],\n","            train_state=train_state,\n","        )\n","\n","        GPUtil.showUtilization()\n","        if is_main_process:\n","            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n","            torch.save(module.state_dict(), file_path)\n","        torch.cuda.empty_cache()\n","\n","        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n","        model.eval()\n","        sloss = run_epoch(\n","            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n","            model,\n","            SimpleLossCompute(module.generator, criterion),\n","            DummyOptimizer(),\n","            DummyScheduler(),\n","            mode=\"eval\",\n","        )\n","        print(sloss)\n","        torch.cuda.empty_cache()\n","\n","    if is_main_process:\n","        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n","        torch.save(module.state_dict(), file_path)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5003,"status":"error","timestamp":1673581563925,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"prCxDL3SCuTn","outputId":"e67be424-4a1f-4b84-e8b8-fb5484995cfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train worker process using GPU: 0 for training\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]},{"name":"stdout","output_type":"stream","text":["[GPU0] Epoch 0 Training ====\n"]},{"ename":"NotImplementedError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-0eaf2f6102a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-0eaf2f6102a5>\u001b[0m in \u001b[0;36mload_trained_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# 如果模型不存在，则调用train_model先训练。这里传入的前四个参数都是\"准备数据\"章节的全局变量。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# 使用词典创建模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-0eaf2f6102a5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# 不启用分布式就直接调用train_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         train_worker(\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         )\n","\u001b[0;32m<ipython-input-36-57dabbd3e12c>\u001b[0m in \u001b[0;36mtrain_worker\u001b[0;34m(gpu, ngpus_per_node, vocab_src, vocab_tgt, spacy_de, spacy_en, config, is_distributed)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[GPU{gpu}] Epoch {epoch} Training ====\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         _, train_state = run_epoch(\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-e60a4f4ee876>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute, optimizer, scheduler, mode, accum_iter, train_state)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# 注意这里的out是decoder输出的结果，这会还没有经过最后一层linear+softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         out = model.forward(\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         )\n","\u001b[0;32m<ipython-input-4-bdf4d79b477b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m\"接收处理屏蔽的src和目标序列\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-bdf4d79b477b>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: "]}],"source":["def train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n","    \"\"\"分布式GPU训练的主入口\n","    \"\"\"\n","    # 译者把源代码下面这一段屏蔽了，train_worker的定义就在上面一个Block，不需要导入了。\n","    # from the_annotated_transformer import train_worker\n","\n","    # mp: 就是import torch.multiprocessing as mp中的多进程启动\n","    # train_worker：每个训练的入口\n","    # nprocs：需要启动的进程数\n","    # args: 每个进程都会收到的参数\n","    ngpus = torch.cuda.device_count()\n","    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n","    os.environ[\"MASTER_PORT\"] = \"12356\"\n","    print(f\"Number of GPUs detected: {ngpus}\")\n","    print(\"Spawning training processes ...\")\n","    # spawn从头构建一个子进程，父进程的数据等拷贝到子进程空间内，拥有自己的Python解释器，\n","    # 所以需要重新加载一遍父进程的包，因此启动较慢，由于数据都是自己的，安全性较高\n","    # 可参考：https://stackoverflow.com/questions/64095876/multiprocessing-fork-vs-spawn\n","    # spawn会将train_worker以(i, args)形式调用，i是master为每个子进程自动分配，所以不用传。\n","    mp.spawn(\n","        train_worker,\n","        nprocs=ngpus,\n","        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n","    )\n","\n","\n","def train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n","    # distributed是用来控制是否分布式训练，如果开启分布式训练，就走第一个if\n","    if config[\"distributed\"]:\n","        train_distributed_model(\n","            vocab_src, vocab_tgt, spacy_de, spacy_en, config\n","        )\n","    # 不启用分布式就直接调用train_worker\n","    else:\n","        train_worker(\n","            0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n","        )\n","\n","\n","def load_trained_model():\n","    config = {\n","        \"batch_size\": 32,\n","        \"distributed\": False, # 这个参数控制是否启动分布式训练\n","        \"num_epochs\": 8,\n","        \"accum_iter\": 10,\n","        \"base_lr\": 1.0,\n","        \"max_padding\": 72,\n","        \"warmup\": 3000,\n","        \"file_prefix\": \"multi30k_model_\",\n","    }\n","    # model保存的位置\n","    model_path = \"multi30k_model_final.pt\"\n","    \n","    # 如果模型不存在，则调用train_model先训练。这里传入的前四个参数都是\"准备数据\"章节的全局变量。\n","    if not exists(model_path):\n","        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n","\n","    # 使用词典创建模型\n","    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n","    # 加载模型参数\n","    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n","    return model\n","\n","\n","model = load_trained_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1673581563926,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"8L17TdKuDU11"},"outputs":[],"source":["def check_outputs(\n","    valid_dataloader,\n","    model,\n","    vocab_src,\n","    vocab_tgt,\n","    n_examples=15,\n","    pad_idx=2,\n","    eos_string=\"</s>\",\n","):\n","    results = [()] * n_examples\n","    for idx in range(n_examples):\n","        print(\"\\nExample %d ========\\n\" % idx)\n","        b = next(iter(valid_dataloader))\n","        rb = Batch(b[0], b[1], pad_idx)\n","        greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\n","\n","        src_tokens = [\n","            vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n","        ]\n","        tgt_tokens = [\n","            vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n","        ]\n","\n","        print(\n","            \"Source Text (Input)        : \"\n","            + \" \".join(src_tokens).replace(\"\\n\", \"\")\n","        )\n","        print(\n","            \"Target Text (Ground Truth) : \"\n","            + \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n","        )\n","        # model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\n","        model_out = beam_decode(model, rb.src, rb.src_mask, 72, 0, BEAM_SIZE=3)\n","        model_txt = (\n","            \" \".join(\n","                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n","            ).split(eos_string, 1)[0]\n","            + eos_string\n","        )\n","        print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))\n","        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n","    return results\n","\n","\n","def run_model_example(n_examples=5):\n","    global vocab_src, vocab_tgt, spacy_de, spacy_en\n","\n","    print(\"Preparing Data ...\")\n","    _, valid_dataloader = create_dataloaders(\n","        torch.device(\"cpu\"),\n","        vocab_src,\n","        vocab_tgt,\n","        spacy_de,\n","        spacy_en,\n","        batch_size=1,\n","        is_distributed=False,\n","    )\n","\n","    print(\"Loading Trained Model ...\")\n","\n","    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n","    model.load_state_dict(\n","        torch.load(\"multi30k_model_final.pt\", map_location=torch.device(\"cpu\"))\n","    )\n","\n","    print(\"Checking Model Outputs:\")\n","    example_data = check_outputs(\n","        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples\n","    )\n","    return model, example_data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1673581563928,"user":{"displayName":"大沛","userId":"06092745297565523916"},"user_tz":-480},"id":"iW4VHRJzDaHV"},"outputs":[],"source":["execute_example(run_model_example)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN2shZhZB4vSq6j7duWibAQ","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.0 (v3.7.0:1bf9cc5093, Jun 26 2018, 23:26:24) \n[Clang 6.0 (clang-600.0.57)]"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
