{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Seq2Seq模型\n",
    "\n",
    "## 1. 模型基础架构\n",
    "\n",
    "### 1.1 模型解释\n",
    "\n",
    "​\t\tSeq2Seq模型可以形象地由下图表示：\n",
    "\n",
    "![img](https://bamboowine-img-1259155549.cos.ap-beijing.myqcloud.com/img/gF2xtP.png)\n",
    "\n",
    "如果将Seq2Seq看作一个黑盒模型，它有三个重要的输入输出变量：\n",
    "\n",
    "+ `enc_input`：编码器Encoder的输入\n",
    "+ `dec_input`：解码器Decoder的输入\n",
    "+ `dec_output`： 解码器Decoder的输出\n",
    "\n",
    "除此之外，还有其他重要的细节：\n",
    "\n",
    "1.  对编码器和解码器的三个重要变量会添加一些 **开始标志** 和 **结束标志**，即 `<SOS>` 和 `<EOS>`；\n",
    "2.  之后会对输入字符串进行转换，转为向量形式；\n",
    "\n",
    "### 1.2 若干疑问\n",
    "\n",
    "+ Decoder的输入和输出，即 `dec_input` 和 `dec_output` 有什么关系？\n",
    "\n",
    "    1) 训练过程，一般情况，`dec_input[t]` 是 t-1时刻解码器的输出；如果采用teacher_forcing，那么`dec_iput`是 `dec_output` 右移一位的结果，这一点从上图中也可以看出来的；2) 测试过程，那么`dec_input[t]` 是 t-1时刻解码器的输出，如下图所示；\n",
    "\n",
    "    <img src=\"https://bamboowine-img-1259155549.cos.ap-beijing.myqcloud.com/img/image-20230104233534346.png\" alt=\"image-20230104233534346\" style=\"zoom:88%;\" />\n",
    "\n",
    "+ 训练和测试过程，Decoder会不会停不下来？\n",
    "\n",
    "    这个是不会的；在训练阶段，目标字符串的长度是已知的；而在测试阶段，是有 **长度限制** 的；\n",
    "\n",
    "## 2. 代码实现\n",
    "\n",
    "### 2.1 导入模块、设置随机数\n",
    "\n",
    "​\t\t导入实验所需的模块，并且设置随机数，保证实验可以复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "same_seed(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 生成数据集\n",
    "\n",
    "​\t\t这个数据集的想法是来源于网络的，可以看作 ** 英文反义词翻译 **，不过他们的想法是一个单词的翻译，就类似下面代码第三行中的 `seq_data`；但是这样的数据集，是没法做测试集的，因为一个崭新的单词，模型肯定是没有见过的，这样输出的结果是毫无意义的。\n",
    "\n",
    "​\t\t所以我们计划对原始的 `seq_data` 两两组合，从原来的9组数据扩展到72组，每组数据是由两个单词组合成的，源字符串和目标字符串仍然是反义词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_data size: 72\n",
      "====================================\n",
      "seq_data: \n",
      "[['girl black', 'boy white'], ['left up', 'right down'], ['man left', 'women right'], ['left fat', 'right thin'], ['high up', 'low down'], ['black small', 'white big'], ['small king', 'big queen'], ['up left', 'down right'], ['up king', 'down queen'], ['fat up', 'thin down'], ['king fat', 'queen thin'], ['girl king', 'boy queen'], ['man king', 'women queen'], ['small girl', 'big boy'], ['high king', 'low queen'], ['king small', 'queen big'], ['fat man', 'thin women'], ['up small', 'down big'], ['king high', 'queen low'], ['left small', 'right big'], ['up fat', 'down thin'], ['small fat', 'big thin'], ['small up', 'big down'], ['small black', 'big white'], ['fat girl', 'thin boy'], ['girl left', 'boy right'], ['small high', 'big low'], ['high left', 'low right'], ['up black', 'down white'], ['small left', 'big right'], ['king man', 'queen women'], ['up girl', 'down boy'], ['left girl', 'right boy'], ['king left', 'queen right'], ['girl high', 'boy low'], ['black left', 'white right'], ['man high', 'women low'], ['king black', 'queen white'], ['man black', 'women white'], ['fat high', 'thin low'], ['high girl', 'low boy'], ['up man', 'down women'], ['fat left', 'thin right'], ['girl small', 'boy big'], ['small man', 'big women'], ['black high', 'white low'], ['king up', 'queen down'], ['king girl', 'queen boy'], ['fat king', 'thin queen'], ['up high', 'down low'], ['high black', 'low white'], ['man girl', 'women boy'], ['high small', 'low big'], ['man fat', 'women thin'], ['fat black', 'thin white'], ['black fat', 'white thin'], ['girl up', 'boy down'], ['left man', 'right women'], ['left high', 'right low'], ['black girl', 'white boy'], ['fat small', 'thin big'], ['black up', 'white down'], ['girl fat', 'boy thin'], ['left black', 'right white'], ['man up', 'women down'], ['black man', 'white women'], ['black king', 'white queen'], ['high fat', 'low thin'], ['girl man', 'boy women'], ['high man', 'low women'], ['man small', 'women big'], ['left king', 'right queen']]\n",
      "====================================\n",
      "vocab_size: 30, max seq_len: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nseq_data size: 72\\n====================================\\nseq_data: \\n[['left girl', 'right boy'], ['fat man', 'thin women'], ['girl black', 'boy white'], ['girl up', 'boy down'], ['black man', 'white women'], ['high black', 'low white'], ['small up', 'big down'], ['black up', 'white down'], ['up girl', 'down boy'], ['girl high', 'boy low'], ['black left', 'white right'], ['up left', 'down right'], ['up man', 'down women'], ['left up', 'right down'], ['high small', 'low big'], ['small man', 'big women'], ['black girl', 'white boy'], ['fat king', 'thin queen'], ['girl left', 'boy right'], ['up black', 'down white'], ['high up', 'low down'], ['man king', 'women queen'], ['fat up', 'thin down'], ['up king', 'down queen'], ['high fat', 'low thin'], ['fat black', 'thin white'], ['left small', 'right big'], ['man left', 'women right'], ['king small', 'queen big'], ['small black', 'big white'], ['king high', 'queen low'], ['black king', 'white queen'], ['girl small', 'boy big'], ['black fat', 'white thin'], ['small high', 'big low'], ['left black', 'right white'], ['small fat', 'big thin'], ['king up', 'queen down'], ['fat small', 'thin big'], ['up fat', 'down thin'], ['small left', 'big right'], ['fat girl', 'thin boy'], ['man up', 'women down'], ['up small', 'down big'], ['girl fat', 'boy thin'], ['king left', 'queen right'], ['king man', 'queen women'], ['left man', 'right women'], ['left high', 'right low'], ['man high', 'women low'], ['high left', 'low right'], ['left king', 'right queen'], ['girl king', 'boy queen'], ['man fat', 'women thin'], ['high girl', 'low boy'], ['high man', 'low women'], ['fat left', 'thin right'], ['high king', 'low queen'], ['small king', 'big queen'], ['black high', 'white low'], ['left fat', 'right thin'], ['black small', 'white big'], ['king girl', 'queen boy'], ['man black', 'women white'], ['girl man', 'boy women'], ['up high', 'down low'], ['small girl', 'big boy'], ['fat high', 'thin low'], ['man girl', 'women boy'], ['man small', 'women big'], ['king fat', 'queen thin'], ['king black', 'queen white']]\\n====================================\\nvocab_size: 30, max seq_len: 11\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter = list('SE?abcdefghijklmnopqrstuvwxyz ')\n",
    "letter2idx = {n: i for i, n in enumerate(letter)}\n",
    "\n",
    "bin_seq = [\n",
    "    ['man', 'women'], ['black', 'white'], ['king', 'queen'],\n",
    "    ['girl', 'boy'], ['up', 'down'], ['high', 'low'],\n",
    "    ['left', 'right'], ['small', 'big'], ['fat', 'thin']]\n",
    "\n",
    "seq_data = []\n",
    "for i in range(len(bin_seq)):\n",
    "    for j in range(i + 1, len(bin_seq)):\n",
    "        seq_data.append([bin_seq[i][0] + ' ' + bin_seq[j][0],\n",
    "                         bin_seq[i][1] + ' ' + bin_seq[j][1]])\n",
    "        seq_data.append([bin_seq[j][0] + ' ' + bin_seq[i][0],\n",
    "                         bin_seq[j][1] + ' ' + bin_seq[i][1]])\n",
    "np.random.shuffle(seq_data)\n",
    "print(f'seq_data size: {len(seq_data)}')\n",
    "print('====================================')\n",
    "print('seq_data: ')\n",
    "print(seq_data)\n",
    "print('====================================')\n",
    "max_len = max([max(len(seq[0]), len(seq[1]))for seq in seq_data])\n",
    "\n",
    "vocab_size = len(letter)\n",
    "print(f'vocab_size: {vocab_size}, max seq_len: {max_len}')\n",
    "\n",
    "\"\"\"\n",
    "seq_data size: 72\n",
    "====================================\n",
    "seq_data: \n",
    "[['left girl', 'right boy'], ['fat man', 'thin women'], ['girl black', 'boy white'], ['girl up', 'boy down'], ['black man', 'white women'], ['high black', 'low white'], ['small up', 'big down'], ['black up', 'white down'], ['up girl', 'down boy'], ['girl high', 'boy low'], ['black left', 'white right'], ['up left', 'down right'], ['up man', 'down women'], ['left up', 'right down'], ['high small', 'low big'], ['small man', 'big women'], ['black girl', 'white boy'], ['fat king', 'thin queen'], ['girl left', 'boy right'], ['up black', 'down white'], ['high up', 'low down'], ['man king', 'women queen'], ['fat up', 'thin down'], ['up king', 'down queen'], ['high fat', 'low thin'], ['fat black', 'thin white'], ['left small', 'right big'], ['man left', 'women right'], ['king small', 'queen big'], ['small black', 'big white'], ['king high', 'queen low'], ['black king', 'white queen'], ['girl small', 'boy big'], ['black fat', 'white thin'], ['small high', 'big low'], ['left black', 'right white'], ['small fat', 'big thin'], ['king up', 'queen down'], ['fat small', 'thin big'], ['up fat', 'down thin'], ['small left', 'big right'], ['fat girl', 'thin boy'], ['man up', 'women down'], ['up small', 'down big'], ['girl fat', 'boy thin'], ['king left', 'queen right'], ['king man', 'queen women'], ['left man', 'right women'], ['left high', 'right low'], ['man high', 'women low'], ['high left', 'low right'], ['left king', 'right queen'], ['girl king', 'boy queen'], ['man fat', 'women thin'], ['high girl', 'low boy'], ['high man', 'low women'], ['fat left', 'thin right'], ['high king', 'low queen'], ['small king', 'big queen'], ['black high', 'white low'], ['left fat', 'right thin'], ['black small', 'white big'], ['king girl', 'queen boy'], ['man black', 'women white'], ['girl man', 'boy women'], ['up high', 'down low'], ['small girl', 'big boy'], ['fat high', 'thin low'], ['man girl', 'women boy'], ['man small', 'women big'], ['king fat', 'queen thin'], ['king black', 'queen white']]\n",
    "====================================\n",
    "vocab_size: 30, max seq_len: 11\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后需要将字符串转为向量形式，就是如下的函数 `word2vec`，采用的方法就是：**用每个字符在letter中的位置索引来代替**；然后需要将所有的字符串填补到 **最大指定长度**，即用 '?' 符号进行尾填充；之后在 `enc_input` 添加 **结束后缀**，`dec_input` 添加 **开始前缀**，`dec_output` 添加 **结束后缀**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(seq_data):\n",
    "    enc_input_all, dec_input_all, dec_output_all = [], [], []\n",
    "    seq_data = copy.deepcopy(seq_data)\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + '?' * (max_len - len(seq[i]))  # 'man??', 'women'\n",
    "\n",
    "        enc_input = [letter2idx[n] for n in (seq[0] + 'E')]\n",
    "        dec_input = [letter2idx[n] for n in ('S' + seq[1])]\n",
    "        dec_output = [letter2idx[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        enc_input_all.append(enc_input)\n",
    "        dec_input_all.append(dec_input)\n",
    "        dec_output_all.append(dec_output)\n",
    "\n",
    "    # make tensor\n",
    "    return torch.LongTensor(enc_input_all), torch.LongTensor(dec_input_all), torch.LongTensor(dec_output_all)\n",
    "\n",
    "# dim: [len(seq_data), max_len+1]\n",
    "# enc_input_all, dec_input_all, dec_output_all = word2vec(seq_data)\n",
    "# enc_input_all.shape, dec_input_all.shape, dec_output_all.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_num_emb = vocab_size        # src 的 vocab_size\n",
    "enc_emb_dim = 16\n",
    "enc_hid_dim = 256\n",
    "enc_num_layers = 2\n",
    "\n",
    "dec_num_emb = vocab_size        # trg 的 vocab_size\n",
    "dec_emb_dim = 16\n",
    "dec_hid_dim = 256\n",
    "dec_num_layers = 2\n",
    "\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "n_early_stop = 1500\n",
    "n_save_steps = 100\n",
    "batch_size = 3\n",
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "train_ratio = 0.9\n",
    "valid_ratio = 0.2\n",
    "save_path = 'model.ckpt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 定义Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, enc_input_all, dec_input_all, dec_output_all) -> None:\n",
    "        super().__init__()\n",
    "        self.enc_input_all = enc_input_all\n",
    "        self.dec_input_all = dec_input_all\n",
    "        self.dec_output_all = dec_output_all\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_input_all[idx], self.dec_input_all[idx], self.dec_output_all[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enc_input_all)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 数据集划分\n",
    "\n",
    "​\t\t这里就是按照上面的参数，将原始数据集划分为 **训练集**、**验证集** 和 **测试集**，同时用 `Dataset` 进行加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 52\n",
      "valid data size: 12\n",
      "test data size: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrain data size: 52\\nvalid data size: 12\\ntest data size: 8\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(seq_data) * train_ratio)\n",
    "train_seq_data, test_seq_data = seq_data[:train_size], seq_data[train_size:]\n",
    "\n",
    "valid_size = int(train_size * valid_ratio)\n",
    "train_seq_data, valid_seq_data = train_seq_data[valid_size:], train_seq_data[:valid_size]\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = SeqDataset(*word2vec(train_seq_data)), \\\n",
    "    SeqDataset(*word2vec(valid_seq_data)), \\\n",
    "    SeqDataset(*word2vec(test_seq_data))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f'train data size: {len(train_seq_data)}')\n",
    "print(f'valid data size: {len(valid_seq_data)}')\n",
    "print(f'test data size: {len(test_seq_data)}')\n",
    "\n",
    "\"\"\"\n",
    "train data size: 52\n",
    "valid data size: 12\n",
    "test data size: 8\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 网络模型\n",
    "\n",
    "#### 2.6.1 编码器\n",
    "\n",
    "​\t\t下面的代码就是编码器的部分，整个流程比较简单：\n",
    "\n",
    "1.  数据进来后，先通过 `nn.Embedding` 层，将之前用索引向量表示的句子转为一个二维向量，类似于one-hot编码；\n",
    "2.  然后是一个GRU模型，需要注意的是，其中的参数 `bidirectional` 默认被设置为 `True`，也就是双向GRU；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, hid_dim, num_layers, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_emb, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers,\n",
    "                          dropout=dropout, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # src: [batch_size, seq_len]\n",
    "    def forward(self, src):\n",
    "        # embedded: [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # embedded: [seq_len, batch_size, emb_dim]\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "\n",
    "        # h0 is zeros, default\n",
    "        # out: [seq_len * D, batch_size, hid_dim]\n",
    "        # hn: [D * num_layers, batch_size, hid_dim]\n",
    "        out, hn = self.rnn(embedded)\n",
    "        return out, hn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 解码器\n",
    "\n",
    "​\t\t解码器的流程：\n",
    "\n",
    "1.  模型的输入有三项：\n",
    "+ `input`：代表前一时刻解码器的**预测结果 **；一般在训练阶段会采用teacher_forcing，那么这时 `input` 可以代表前一时刻的**实际结果 **，也就是上面`Dataset`中的 `dec_input_all`；值得注意的就是在第一时刻，`input` 是上文提到的 ** 开始前缀**；\n",
    "+ `hidden`：代表前一时刻解码器的隐层状态；在第一时刻，`hidden`是**编码器的隐层状态输出**；\n",
    "+ `context`：代表编码器得到的 ** 上下文向量**；\n",
    "2.  首先还是 `input` 会经过一个 `nn.Embeddeding`层；\n",
    "3.  然后 `context` 会和 `input` 拼接，作为当前时刻解码器的输入 ；\n",
    "4.  最后是一个线性层，进行维度转换，从 `hid_dim` 到 `num_emb`；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, hid_dim, num_layers, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_emb, emb_dim)\n",
    "        # self.rnn = nn.GRU(emb_dim, hid_dim, num_layers, dropout=dropout)\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim, num_layers,\n",
    "                          dropout=dropout, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, num_emb)\n",
    "\n",
    "    # input:    [batch_size,]\n",
    "    # hidden:   [num_layers, batch_size, hid_dim]\n",
    "    # context:  [1, batch_size, hid_dim]\n",
    "    def forward(self, input, hidden, context):\n",
    "        # embedded: [batch_size, emb_dim]\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # embedded: [1, batch_size, emb_dim]\n",
    "        embedded = embedded.unsqueeze(0)\n",
    "\n",
    "        # emb_cxt_cat: [1, batch_size, emb_dim + hid_dim]\n",
    "        emb_cxt_cat = torch.cat((embedded, context), dim=-1)\n",
    "\n",
    "        # out: [1, batch_size, hid_dim]\n",
    "        # hn: [num_layers, batch_size, hid_dim]\n",
    "        out, hn = self.rnn(emb_cxt_cat, hidden)\n",
    "\n",
    "        # out: [batch_size, num_emb]\n",
    "        out = self.fc_out(out.squeeze(0))\n",
    "        return out, hn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.3 seq2seq\n",
    "\n",
    "​\t\tseq2seq模型的工作是将Encoder和Decoder进行结合：\n",
    "\n",
    "1.  模型的输入有三项：\n",
    "    + `src`：源字符串；\n",
    "    + `trg`：目标字符串；在训练阶段是 **正确的目标字符串**，在测试阶段是 **一段最大长度的<pad>填充的字符串**；\n",
    "    + `teacher_forcing_ratio`：解码器输入的teacher_forcing概率；\n",
    "2.  首先将 `input` 输入到编码器，得到 `enc_out` 和 `hidden`，在demo中，令最后一个时刻的 `enc_out` 输出作为 上下文向量 `context`，即 `context = enc_out[-1:]`；\n",
    "3.  然后由于编码器是双向的，而解码器采用单向的，所以编码器的输出 `enc_out` 和 `hidden` 维度和解码器的输入维度不对等，因此需要加一个线性层进行维度转换；\n",
    "4.  之后解码器的工作流程是由 **for循环迭代** 完成的，最后得到预测的预测结果 `dec_outs`；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "        # Encoder\n",
    "        if bidirectional:\n",
    "            self.hid_tran = nn.Linear(\n",
    "                encoder.num_layers * 2, decoder.num_layers)\n",
    "            self.out_tran = nn.Linear(encoder.hid_dim * 2, decoder.hid_dim)\n",
    "\n",
    "    # src: [batch_size, src_len]\n",
    "    # trg: [batch_size, trg_len]\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        num_trg_vocab = self.decoder.num_emb\n",
    "        enc_out, hidden = self.encoder(src)\n",
    "        context = enc_out[-1:]\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden = self.hid_tran(hidden.permute(\n",
    "                1, 2, 0)).permute(2, 0, 1).contiguous()\n",
    "            context = self.out_tran(context)\n",
    "\n",
    "        dec_outs = torch.zeros(trg_len, batch_size,\n",
    "                               num_trg_vocab).to(self.device)\n",
    "\n",
    "        # dec_input：[batch_size,]\n",
    "        dec_input = trg[:, 0]\n",
    "        for t in range(0, trg_len):\n",
    "            # out: [batch_size, num_trg_vocab]\n",
    "            out, hidden = self.decoder(dec_input, hidden, context)\n",
    "            dec_outs[t] = out\n",
    "            pred = out.argmax(1)\n",
    "            dec_input = trg[:, t] if np.random.random(\n",
    "            ) < teacher_forcing_ratio else pred\n",
    "        return dec_outs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 训练过程\n",
    "\n",
    "​\t\t整个训练过程中规中矩，采用 CrossEntropyLoss 的 criterion，然后在计算每个batch的损失值是由for循环计算每组数据的loss再累加得到的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000]: Train loss: 0.01967122, Valid loss: 0.06276214\n",
      "Saving model with loss 0.06276214...\n",
      "Epoch [200/5000]: Train loss: 0.06223520, Valid loss: 0.02086368\n",
      "Saving model with loss 0.02086368...\n",
      "Epoch [300/5000]: Train loss: 0.00187870, Valid loss: 0.01757691\n",
      "Saving model with loss 0.01757691...\n",
      "Epoch [400/5000]: Train loss: 0.00505274, Valid loss: 0.01616128\n",
      "Saving model with loss 0.01616128...\n",
      "Epoch [500/5000]: Train loss: 0.06280922, Valid loss: 0.05333685\n",
      "Epoch [600/5000]: Train loss: 0.00925549, Valid loss: 0.20344227\n",
      "Epoch [700/5000]: Train loss: 0.10506145, Valid loss: 0.12112796\n",
      "Epoch [800/5000]: Train loss: 0.01458156, Valid loss: 0.09561630\n",
      "Epoch [900/5000]: Train loss: 0.00054563, Valid loss: 0.14745553\n",
      "Epoch [1000/5000]: Train loss: 0.00074608, Valid loss: 0.21140787\n",
      "Epoch [1100/5000]: Train loss: 0.01201170, Valid loss: 0.04874226\n",
      "Epoch [1200/5000]: Train loss: 0.06151213, Valid loss: 0.27386813\n",
      "Epoch [1300/5000]: Train loss: 0.00314407, Valid loss: 0.09637893\n",
      "Epoch [1400/5000]: Train loss: 0.00284008, Valid loss: 0.09839174\n",
      "Epoch [1500/5000]: Train loss: 0.02108082, Valid loss: 0.02641225\n",
      "Epoch [1600/5000]: Train loss: 0.05660263, Valid loss: 0.07271549\n",
      "Epoch [1700/5000]: Train loss: 0.00050096, Valid loss: 0.09169701\n",
      "Epoch [1800/5000]: Train loss: 0.00132486, Valid loss: 0.00451173\n",
      "Saving model with loss 0.00451173...\n",
      "Epoch [1900/5000]: Train loss: 0.00462252, Valid loss: 0.10329686\n",
      "Epoch [2000/5000]: Train loss: 0.00049926, Valid loss: 0.08813906\n",
      "Epoch [2100/5000]: Train loss: 0.01426757, Valid loss: 0.01679915\n",
      "Epoch [2200/5000]: Train loss: 0.00153950, Valid loss: 0.12600733\n",
      "Epoch [2300/5000]: Train loss: 0.02287119, Valid loss: 0.06641972\n",
      "Epoch [2400/5000]: Train loss: 0.00718637, Valid loss: 0.09846697\n",
      "Epoch [2500/5000]: Train loss: 0.04036630, Valid loss: 0.49282018\n",
      "Epoch [2600/5000]: Train loss: 0.00194282, Valid loss: 0.08817798\n",
      "Epoch [2700/5000]: Train loss: 0.01108271, Valid loss: 0.05519202\n",
      "Epoch [2800/5000]: Train loss: 0.01955757, Valid loss: 0.20318323\n",
      "Epoch [2900/5000]: Train loss: 0.01177116, Valid loss: 0.08833868\n",
      "Epoch [3000/5000]: Train loss: 0.02287891, Valid loss: 0.22166587\n",
      "Epoch [3100/5000]: Train loss: 0.00137642, Valid loss: 0.16324030\n",
      "Epoch [3200/5000]: Train loss: 0.00225637, Valid loss: 0.01936966\n",
      "Epoch [3300/5000]: Train loss: 0.02961215, Valid loss: 0.19348846\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Epoch [3400/5000]: Train loss: 0.08176930, Valid loss: 0.09088761\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Epoch [3500/5000]: Train loss: 0.10568509, Valid loss: 0.09654379\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "\n",
      "Best valid loss: 0.0045117263798601925\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Epoch [3600/5000]: Train loss: 0.00845043, Valid loss: 0.00185200\n",
      "Saving model with loss 0.00185200...\n",
      "Epoch [3700/5000]: Train loss: 0.01110618, Valid loss: 0.05092434\n",
      "Epoch [3800/5000]: Train loss: 0.08366504, Valid loss: 0.28908812\n",
      "Epoch [3900/5000]: Train loss: 0.04126409, Valid loss: 0.03926808\n",
      "Epoch [4000/5000]: Train loss: 0.00555797, Valid loss: 0.21074286\n",
      "Epoch [4100/5000]: Train loss: 0.00683675, Valid loss: 0.17083344\n",
      "Epoch [4200/5000]: Train loss: 0.00068279, Valid loss: 0.08804119\n",
      "Epoch [4300/5000]: Train loss: 0.00204416, Valid loss: 0.06154456\n",
      "Epoch [4400/5000]: Train loss: 0.00205050, Valid loss: 0.19405193\n",
      "Epoch [4500/5000]: Train loss: 0.01624802, Valid loss: 0.52775419\n",
      "Epoch [4600/5000]: Train loss: 0.02726205, Valid loss: 0.17253668\n",
      "Epoch [4700/5000]: Train loss: 0.00704153, Valid loss: 0.04284485\n",
      "Epoch [4800/5000]: Train loss: 0.10486065, Valid loss: 0.19463341\n",
      "Epoch [4900/5000]: Train loss: 0.01359521, Valid loss: 0.16213919\n",
      "Epoch [5000/5000]: Train loss: 0.00165762, Valid loss: 0.51407535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch [100/5000]: Train loss: 0.02501816, Valid loss: 0.05040905\\nSaving model with loss 0.05040905...\\nEpoch [200/5000]: Train loss: 0.11218390, Valid loss: 0.13767572\\nEpoch [300/5000]: Train loss: 0.00292161, Valid loss: 0.00804337\\nSaving model with loss 0.00804337...\\nEpoch [400/5000]: Train loss: 0.06505425, Valid loss: 0.06209812\\nEpoch [500/5000]: Train loss: 0.00547583, Valid loss: 0.01514314\\nEpoch [600/5000]: Train loss: 0.00443477, Valid loss: 0.07757122\\nEpoch [700/5000]: Train loss: 0.00155966, Valid loss: 0.05722510\\nEpoch [800/5000]: Train loss: 0.00427389, Valid loss: 0.07414128\\nEpoch [900/5000]: Train loss: 0.18153160, Valid loss: 0.12175162\\nEpoch [1000/5000]: Train loss: 0.00124802, Valid loss: 0.01263226\\nEpoch [1100/5000]: Train loss: 0.02267172, Valid loss: 0.14032485\\nEpoch [1200/5000]: Train loss: 0.00175910, Valid loss: 0.01846752\\nEpoch [1300/5000]: Train loss: 0.00211835, Valid loss: 0.02610047\\nEpoch [1400/5000]: Train loss: 0.00064308, Valid loss: 0.14011183\\nEpoch [1500/5000]: Train loss: 0.00622014, Valid loss: 0.02119574\\nEpoch [1600/5000]: Train loss: 0.00481345, Valid loss: 0.01523455\\nEpoch [1700/5000]: Train loss: 0.00129550, Valid loss: 0.35570633\\nEpoch [1800/5000]: Train loss: 0.00097497, Valid loss: 0.02685569\\n\\nBest valid loss: 0.008043368288781494\\n\\nModel is not improving, so we halt the training session.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main(train_loader, valid_loader, criterion):\n",
    "    optimizer = optim.Adam(seq2seq.parameters(), lr=lr)\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=16, T_mult=1)\n",
    "\n",
    "    best_loss, early_stop = np.inf, 0\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_record = []\n",
    "        seq2seq.train()\n",
    "        for enc_input_batch, dec_input_batch, dec_out_batch in train_loader:\n",
    "            enc_input_batch, dec_input_batch, dec_out_batch = enc_input_batch.to(\n",
    "                device), dec_input_batch.to(device), dec_out_batch.to(device)\n",
    "            pred = seq2seq(enc_input_batch, dec_input_batch,\n",
    "                           teacher_forcing_ratio)\n",
    "            pred = pred.transpose(0, 1)\n",
    "\n",
    "            loss = 0\n",
    "            for i in range(len(dec_out_batch)):\n",
    "                loss += criterion(pred[i], dec_out_batch[i])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "\n",
    "        mean_train_loss = sum(loss_record) / len(loss_record)\n",
    "\n",
    "        seq2seq.eval()\n",
    "        loss_record = []\n",
    "        with torch.no_grad():\n",
    "            for enc_input_batch, dec_input_batch, dec_out_batch in valid_loader:\n",
    "                enc_input_batch, dec_input_batch, dec_out_batch = enc_input_batch.to(\n",
    "                    device), dec_input_batch.to(device), dec_out_batch.to(device)\n",
    "                pred = seq2seq(enc_input_batch, dec_input_batch,\n",
    "                               teacher_forcing_ratio)\n",
    "                pred = pred.transpose(0, 1)\n",
    "                loss = 0\n",
    "                for i in range(len(dec_out_batch)):\n",
    "                    loss += criterion(pred[i], dec_out_batch[i])\n",
    "                loss_record.append(loss.item())\n",
    "\n",
    "        mean_valid_loss = sum(loss_record) / len(loss_record)\n",
    "\n",
    "        if (epoch + 1) % n_save_steps == 0:\n",
    "            print(\n",
    "                f'Epoch [{epoch + 1}/{n_epochs}]: Train loss: {mean_train_loss:.8f}, Valid loss: {mean_valid_loss:.8f}')\n",
    "            if mean_valid_loss < best_loss:\n",
    "                best_loss = mean_valid_loss\n",
    "                torch.save(seq2seq.state_dict(), save_path)\n",
    "                print(f'Saving model with loss {best_loss:.8f}...')\n",
    "                early_stop = 0\n",
    "            else:\n",
    "                early_stop += n_save_steps\n",
    "\n",
    "        if early_stop >= n_early_stop:\n",
    "            print(f'\\nBest valid loss: {best_loss}')\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "\n",
    "\n",
    "encoder = Encoder(enc_num_emb, enc_emb_dim, enc_hid_dim,\n",
    "                  enc_num_layers, dropout).to(device)\n",
    "decoder = Decoder(dec_num_emb, dec_emb_dim, dec_hid_dim,\n",
    "                  dec_num_layers, dropout).to(device)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "main(train_loader, valid_loader, criterion)\n",
    "\n",
    "\"\"\"\n",
    "Epoch [100/5000]: Train loss: 0.02501816, Valid loss: 0.05040905\n",
    "Saving model with loss 0.05040905...\n",
    "Epoch [200/5000]: Train loss: 0.11218390, Valid loss: 0.13767572\n",
    "Epoch [300/5000]: Train loss: 0.00292161, Valid loss: 0.00804337\n",
    "Saving model with loss 0.00804337...\n",
    "Epoch [400/5000]: Train loss: 0.06505425, Valid loss: 0.06209812\n",
    "Epoch [500/5000]: Train loss: 0.00547583, Valid loss: 0.01514314\n",
    "Epoch [600/5000]: Train loss: 0.00443477, Valid loss: 0.07757122\n",
    "Epoch [700/5000]: Train loss: 0.00155966, Valid loss: 0.05722510\n",
    "Epoch [800/5000]: Train loss: 0.00427389, Valid loss: 0.07414128\n",
    "Epoch [900/5000]: Train loss: 0.18153160, Valid loss: 0.12175162\n",
    "Epoch [1000/5000]: Train loss: 0.00124802, Valid loss: 0.01263226\n",
    "Epoch [1100/5000]: Train loss: 0.02267172, Valid loss: 0.14032485\n",
    "Epoch [1200/5000]: Train loss: 0.00175910, Valid loss: 0.01846752\n",
    "Epoch [1300/5000]: Train loss: 0.00211835, Valid loss: 0.02610047\n",
    "Epoch [1400/5000]: Train loss: 0.00064308, Valid loss: 0.14011183\n",
    "Epoch [1500/5000]: Train loss: 0.00622014, Valid loss: 0.02119574\n",
    "Epoch [1600/5000]: Train loss: 0.00481345, Valid loss: 0.01523455\n",
    "Epoch [1700/5000]: Train loss: 0.00129550, Valid loss: 0.35570633\n",
    "Epoch [1800/5000]: Train loss: 0.00097497, Valid loss: 0.02685569\n",
    "\n",
    "Best valid loss: 0.008043368288781494\n",
    "\n",
    "Model is not improving, so we halt the training session.\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 测试\n",
    "\n",
    "​\t\t测试阶段的流程：\n",
    "\n",
    "1.  我们首先实现了 `predict` 函数，它的输入是源字符串，输出是预测字符串；\n",
    "2.  为了满足seq2seq模型的输入，我们需要构造一个 “假的” 目标字符串，由 `max_len` 个 ？组成，这也是 **预测阶段不会无法停止** 的重要原因！！！\n",
    "3.  通过seq2seq模型，得到预测输出 `out`，然后选择最大值的索引，形成索引向量；之后通过 `letter` 将索引向量再转为字符串列表，然后以 `E` 或者 `?` 第一次出现的位置进行截断；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  man up    -> women down \n",
      " black man  -> white women\n",
      "black king  -> white duwn \n",
      " high fat   ->  low thin  \n",
      " girl man   ->  boy women \n",
      " high man   ->  low women \n",
      " man small  ->  women big \n",
      " left king  -> right women\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n  man up    -> women down \\n black man  -> white women\\nblack king  -> white queen\\n high fat   ->  low thin  \\n girl man   ->  boy women \\n high man   ->  low women \\n man small  ->  women big \\n left king  -> right queen\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(enc_num_emb, enc_emb_dim, enc_hid_dim,\n",
    "                  enc_num_layers, dropout).to(device)\n",
    "decoder = Decoder(dec_num_emb, dec_emb_dim, dec_hid_dim,\n",
    "                  dec_num_layers, dropout).to(device)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "seq2seq.load_state_dict(torch.load(save_path))\n",
    "seq2seq.eval()\n",
    "\n",
    "\n",
    "def index(s, c):\n",
    "    return s.index(c) if c in s else len(s)\n",
    "\n",
    "\n",
    "def predict(word: str):\n",
    "    enc_input, dec_input, _ = word2vec([[word, '?' * max_len]])\n",
    "    enc_input, dec_input = enc_input.to(device), dec_input.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = seq2seq(enc_input, dec_input, teacher_forcing_ratio=0.0)\n",
    "    out = out.squeeze(1).argmax(1)\n",
    "    pred = ''.join([letter[i] for i in out])\n",
    "    end = min(index(pred, 'E'), index(pred, '?'))\n",
    "    pred = pred[:end]\n",
    "    return pred\n",
    "\n",
    "\n",
    "for seq in test_seq_data:\n",
    "    src, trg = seq\n",
    "    pred = predict(src)\n",
    "    print(f\"{src:^11} -> {pred:^11}\")\n",
    "\n",
    "\"\"\"\n",
    "  man up    -> women down \n",
    " black man  -> white women\n",
    "black king  -> white queen\n",
    " high fat   ->  low thin  \n",
    " girl man   ->  boy women \n",
    " high man   ->  low women \n",
    " man small  ->  women big \n",
    " left king  -> right queen\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Layer\n",
    "\n",
    "​\t\t之前的Seq2Seq模型中Encoder是一个RNN，因此在输入序列很少的时候，Encoder最终会或多会少遗忘部分信息，然后Decoder可能无法生成正确的输出结果。解决该缺点最有效的方法是 **Attention**：**Decoder每次更新状态时会查看Encoder所有的状态，让Decoder关注Encoder中最相关的信息，从而避免遗忘**。\n",
    "\n",
    "​\t\t添加Attention layer的工作流程：\n",
    "\n",
    "+ 在Encoder对输入序列编码结束之后（**保存所有的状态$h_1, h_2, \\ldots, h_m$**），Attention和Decoder同时工作；\n",
    "\n",
    "+ 根据Decoder当前时刻的状态$s_{t}$，Attention会计算其与Encoder所有状态的相关性 $\\alpha_{t1},\\alpha_{t2},\\ldots, \\alpha_{tm}$，满足\n",
    "    $$\n",
    "    \\sum_{i=1}^m \\alpha_{ti} = 1\n",
    "    $$\n",
    "    这一点无疑可以通过Softmax来实现。\n",
    "\n",
    "+ 将相关性和Encoder的所有状态进行加权平均，得到Context vector，记为 $c_t$\n",
    "    $$\n",
    "    c_t = \\alpha_{t1}h_1 + \\alpha_{t2}h_2 + \\cdots + \\alpha_{tm}h_m\n",
    "    $$\n",
    "\n",
    "+ 然后上下文向量 $c_t$ 和 当前时刻embedding进行拼接，作为Decoder的输入。\n",
    "\n",
    "然后现在的核心问题就是 **Attention如何计算相关性权重**，这里介绍两种方法，首先<b><font color=\"red\">第一种</font></b>，如下图所示。将Encoder中隐藏层状态 $h_i$ 和解码器当前状态 $s_t$ 拼接，然后左乘参数矩阵 $W$ 得到一个向量；之后应用双曲正切函数tanh在得到的向量上，将元素值调整到-1到1之间，最后再和参数向量 $V$ 进行点积运算，记为 $\\tilde{a}_{ti}$。计算出 $\\tilde{a}_{t1}, \\tilde{a}_{t2}, \\dots, \\tilde{a}_{tm}$，进行Softmax变换，得到 ${a}_{t1}, {a}_{t2}, \\dots, {a}_{tm}$。\n",
    "\n",
    "![image-20230108184250154](https://bamboowine-img-1259155549.cos.ap-beijing.myqcloud.com/img/image-20230108184250154.png)\n",
    "\n",
    "这里再介绍<b><font color=\"red\">另一种方法</font></b>：\n",
    "\n",
    "1. 分别用两个参数矩阵 $W_k$ 和 $W_Q$ 对 $h_i$ 和 $s_t$ 进行线性变换，得到向量 $k_i$ 和 $q_t$：\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    k_i = W_k \\cdot h_i, \\quad \\text{for i = 1 to m} \\\\\n",
    "    q_t = W_Q \\cdot s_t, \\quad \\text{for t = 1 to T}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "2. 计算向量 $k_i$ 与 $q_t$ 的内积，得到 $\\tilde{\\alpha}_{ti}$：\n",
    "    $$\n",
    "    \\tilde{\\alpha}_{ti} = k_i^Tq_t, \\quad \\text{for i = 1 to m,} \\,\\, \\text{t = 0 to T}\n",
    "    $$\n",
    "\n",
    "3. 进行Softmax变换，得到 ${a}_{t1}, {a}_{t2}, \\dots, {a}_{tm}$。\n",
    "\n",
    "<b><font color=\"red\"> 本Demo采用的是第一种</font></b>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoder\n",
    "\n",
    "​\t\t编码器部分和之前的是一样的，因为主要在**解码器、以及与编码器连接的部分**需要改动。\n",
    "\n",
    "### 3.2 Attention\n",
    "\n",
    "​\t\tAttention部分的代码比较简单，输出值是**相关性权重**；构造参数是编码器和解码器的隐层维度，forward函数的输入是解码器当前状态 `s` 和编码器输出 `enc_out` (也就是隐层状态)，然后就是将 `s` 和 `enc_out` 进行拼接，首先是需要将 `s` 进行 `repeat`操作，对 `enc_out` 维度适当调整，接下来的步骤和前面介绍的完全一致。这里需要注意的是参数矩阵 `atten` 的维度，这里 `(enc_hid_dim * 2)` 的原因是 **Encoder采用的是双向的**，而另一个维度 `dec_hid_dim` 则是随机指定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.atten = nn.Linear((enc_hid_dim * 2) +\n",
    "                               dec_hid_dim, dec_hid_dim, bias=False)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    # s:   \t\t[batch_size, dec_hid_dim]\n",
    "    # enc_out:  [seq_len, batch_size, enc_hid_dim * D(2)]\n",
    "    def forward(self, s, enc_out):\n",
    "        batch_size, seq_len = enc_out.shape[1], enc_out.shape[0]\n",
    "        # s:   [batch_size, seq_len, dec_hid_dim]\n",
    "        # enc_out:  [batch_size, seq_len, enc_hid_dim * D(2)]\n",
    "        s = s.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        enc_out = enc_out.transpose(0, 1)\n",
    "\n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "        energy = torch.tanh(self.atten(torch.cat((s, enc_out), dim=2)))\n",
    "        # attention: [batch_size, seq_len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Decoder\n",
    "\n",
    "​\t\t网上有关这一部分的代码和我的不要一样，主要原因在于 **他们的GRU是一层的，但我可以是多层的**；我浅说一下这里的设计：\n",
    "\n",
    "+ 首先由于可以采用多层GRU，所以它的隐层状态就是多层的，为了得到Attention中的 `s`，我们可以对隐层状态进行 类似**池化**操作，我这里是通过一个线性层完成的，也就是 `fc_hidden`；\n",
    "+ 将 `s` 和 `enc_out` 输入到 Attention，得到相关性权重 `a`；\n",
    "+ 之后将 `a` 和 `enc_out` 相乘，得到上下文向量 `c`；\n",
    "+ `c` 和 `embedding` 连接，输入到GRU中；\n",
    "\n",
    "其余的操作和之前是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, enc_hid_dim, dec_hid_dim, num_layers, dropout, attention) -> None:\n",
    "        super().__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.hid_dim = dec_hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(num_emb, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + (enc_hid_dim) * 2, dec_hid_dim,\n",
    "                          num_layers, dropout=dropout, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_hidden = nn.Linear(num_layers, 1)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, num_emb)\n",
    "\n",
    "    # input:    [batch_size,]\n",
    "    # hidden:   [num_layers, batch_size, dec_hid_dim]\n",
    "    # enc_out:  [seq_len, batch_size, D * enc_hid_dim]\n",
    "    def forward(self, input, hidden, enc_out):\n",
    "        # embedded: [batch_size, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        # embedded: [1, batch_size, emb_dim]\n",
    "        embedded = embedded.unsqueeze(0)\n",
    "\n",
    "        s = self.fc_hidden(hidden.permute(1, 2, 0)).permute(\n",
    "            2, 0, 1).contiguous()\n",
    "        s = s.squeeze(0)\n",
    "\n",
    "        # a: [batch_size, 1, src_len]\n",
    "        a = self.attention(s, enc_out).unsqueeze(1)\n",
    "\n",
    "        # enc_out: [batch_size, src_len, enc_hid_dim * 2]\n",
    "        enc_out = enc_out.transpose(0, 1)\n",
    "\n",
    "        # c: [1, batch_size, enc_hid_dim * 2]\n",
    "        c = torch.bmm(a, enc_out).transpose(0, 1)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, c), dim=2)\n",
    "\n",
    "        # out: [1, batch_size, dec_hid_dim]\n",
    "        # hn: [num_layers, batch_size, dec_hid_dim]\n",
    "        out, hn = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        # out: [batch_size, num_emb]\n",
    "        out = self.fc_out(torch.cat((out.squeeze(0), c.squeeze(0)), dim=1))\n",
    "        return out, hn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Seq2Seq\n",
    "\n",
    "​\t\t解码器部分和之前几乎是完全相同的，依旧**对Encoder的最终隐层状态进行线性变换，得到Decoder隐层输入的维度**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.fc = nn.Linear(encoder.hid_dim * 2, decoder.hid_dim)\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "        # Encoder\n",
    "        if bidirectional:\n",
    "            self.fc_hidden = nn.Linear(\n",
    "                encoder.num_layers * 2, decoder.num_layers)\n",
    "\n",
    "    # src: [batch_size, src_len]\n",
    "    # trg: [batch_size, trg_len]\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        num_trg_vocab = self.decoder.num_emb\n",
    "        enc_out, hidden = self.encoder(src)\n",
    "\n",
    "        # hidden: [num_layers, batch_size, hid_dim]\n",
    "        if bidirectional:\n",
    "            hidden = self.fc_hidden(hidden.permute(\n",
    "                1, 2, 0)).permute(2, 0, 1).contiguous()\n",
    "\n",
    "        dec_outs = torch.zeros(trg_len, batch_size,\n",
    "                               num_trg_vocab).to(self.device)\n",
    "\n",
    "        # dec_input: [batch_size,]\n",
    "        dec_input = trg[:, 0]\n",
    "        for t in range(0, trg_len):\n",
    "            # out: [batch_size, num_trg_vocab]\n",
    "            out, hidden = self.decoder(dec_input, hidden, enc_out)\n",
    "            dec_outs[t] = out\n",
    "            pred = out.argmax(1)\n",
    "            dec_input = trg[:, t] if np.random.random(\n",
    "            ) < teacher_forcing_ratio else pred\n",
    "        return dec_outs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 测试效果\n",
    "\n",
    "​\t\t可能由于数据集比较小，所以之前的基于RNN的Seq2Seq模型已经可以取得非常好的效果，基于RNN和Attention的效果看不出明显的提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngirl black  ->  boy white \\n  left up   -> right down \\n man left   -> women right\\n left fat   -> right thin \\n  high up   ->  low down  \\nblack small ->  white big \\nsmall king  ->  big queen \\n  up left   -> down right \\n  up king   -> down queen \\n  fat up    ->  thin down \\n king fat   -> queen thin \\n girl king  ->  boy queen \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "girl black  ->  boy white \n",
    "  left up   -> right down \n",
    " man left   -> women right\n",
    " left fat   -> right thin \n",
    "  high up   ->  low down  \n",
    "black small ->  white big \n",
    "small king  ->  big queen \n",
    "  up left   -> down right \n",
    "  up king   -> down queen \n",
    "  fat up    ->  thin down \n",
    " king fat   -> queen thin \n",
    " girl king  ->  boy queen \n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 遇到的问题\n",
    "\n",
    "+ 无论是之前的Seq2Seq，还是基于Attention的Seq2Seq，当 `num_layers = 1` 时，模型的效果总不是很理想，当 `num_layers` 设置为 2 的时候，就可以取得非常好的效果。\n",
    "\n",
    "## 5. 参考资料\n",
    "\n",
    "1. [Seq2Seq 的 PyTorch 实现]([Seq2Seq的PyTorch实现 - mathor (wmathor.com)](https://wmathor.com/index.php/archives/1448/))\n",
    "2. [pytorch中如何做seq2seq]([pytorch中如何做seq2seq - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/352276786))\n",
    "3. [基于pytorch的Seq2Seq的实现]([基于pytorch的Seq2Seq的实现_loki2018的博客-CSDN博客_pytorch seq2seq](https://blog.csdn.net/loki2018/article/details/118071500))\n",
    "4. [Seq2Seq(Attention)的PyTorch实现（超级详细）]([Seq2Seq(Attention)的PyTorch实现（超级详细）_数学家是我理想的博客-CSDN博客_seq2seq pytorch实现](https://blog.csdn.net/qq_37236745/article/details/107085532))\n",
    "5. [Attention is all you need：剥离RNN，保留Attention]([Attention is all you need：剥离RNN，保留Attention_DeepGeGe的博客-CSDN博客](https://blog.csdn.net/qq_24178985/article/details/118727611))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (v3.7.0:1bf9cc5093, Jun 26 2018, 23:26:24) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
